[{"title":"Scrut","type":0,"sectionRef":"#","url":"/scrut/docs/","content":"","keywords":"","version":"Next"},{"title":"Contribute‚Äã","type":1,"pageTitle":"Scrut","url":"/scrut/docs/#contribute","content":"CONTRIBUTING.mdCODE_OF_CONDUCT.md ","version":"Next","tagName":"h2"},{"title":"License‚Äã","type":1,"pageTitle":"Scrut","url":"/scrut/docs/#license","content":"LICENSE ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/scrut/docs/advanced/development/","content":"","keywords":"","version":"Next"},{"title":"Use Cases‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#use-cases","content":" Scrut is a command line program that solves the following use-cases for developers / owners of command line programs:   &lt;Mermaid chart={` graph TD user(CLI Owner) create[Create Tests] update[Update Tests] run[Run Tests] cicd(CI/CD) user -- manual --&gt; create user -- manual --&gt; update user -- manual --&gt; run user -- automated --&gt; run cicd -- automated ---&gt; run `} /&gt;   ","version":"Next","tagName":"h2"},{"title":"Create Tests‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#create-tests","content":" Make it easy for owners to create tests for their CLIs. Accept arbitrary commands (or more complex shell expressions), execute them and create formatted tests from the resulting output.  Test Case generation is described by the TestCaseGenerator trait and implemented in the respective format in the same folder.  ","version":"Next","tagName":"h3"},{"title":"Update Tests‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#update-tests","content":" Make it easy for owners to maintain the tests of their CLIs. Automate update of previously created test files when the expected output changes.  The generation of the update is described by the UpdateGenerator trait and implemented in the respective format in the same folder.  ","version":"Next","tagName":"h3"},{"title":"Run Tests‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#run-tests","content":" Run previously persisted tests, so to prove that a CLI works within expectations. Owners can do this either manually, or automated from integration with their development tooling. The same tests should be run by automated continuous integration systems.  ","version":"Next","tagName":"h3"},{"title":"Architecture‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#architecture","content":" The architecture of Scrut is best explained by following the process flow of the primary use case: executing tests.   &lt;Mermaid chart={` flowchart TB TestCase[&quot;Test Case&quot;] TestCases[&quot;Test Case(s)&quot;] Expectations[&quot;Expectation(s)&quot;] DiffTool[&quot;Diff Tool&quot;] DocumentConfiguration[&quot;Document Config&quot;] TestCaseConfiguration[&quot;TestCase Config&quot;] subgraph Parsing[&quot;Phase: Parsing&quot;] Run ---&gt; Parser Parser ---&gt; TestCases Parser ---&gt; DocumentConfiguration end subgraph Anatomy[&quot;Test Case in Detail&quot;] TestCase ---&gt; ShellExpression TestCase ---&gt; TestCaseConfiguration TestCase ---&gt; Expectations TestCase .-&gt; Title end TestCases .-&gt; TestCase subgraph Execution[&quot;Phase: Execution&quot;] Executor ---&gt; Output end DocumentConfiguration ---&gt; Executor TestCaseConfiguration ---&gt; Executor ShellExpression ---&gt; Executor subgraph Validation[&quot;Phase: Validation&quot;] DiffTool -- expected output ---&gt; OK DiffTool -- unexpected output ---&gt; Error end subgraph Presentation[&quot;Phase: Presentation&quot;] OK ---&gt; Renderer Error ---&gt; Renderer Renderer ---&gt; Diff[&quot;Pretty, human\\nreadable differences&quot;] Renderer ---&gt; Patch[&quot;Universal Diff Format&quot;] Renderer ---&gt; YAML Renderer ---&gt; JSON end Output ---&gt; DiffTool Expectations ---&gt; DiffTool style Anatomy fill:#eee,stroke:#aaa; style TestCase fill:#ddd,stroke:#aaa; style TestCaseConfiguration fill:#ddd,stroke:#aaa; style ShellExpression fill:#ddd,stroke:#aaa; style Expectations fill:#ddd,stroke:#aaa; style Title fill:#ddd,stroke:#aaa; style OK fill:#8f8; style Error fill:#f88; `} /&gt;   ","version":"Next","tagName":"h2"},{"title":"Phase: Parsing‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#phase-parsing","content":" Scrut tests are stored either in Markdown or Cram files. Each file can contain multiple tests, which are called Test Cases and which consist of:  Title that explains to a human what this case is intended to proveShell Expression is an arbitrary command or multiple chained commands, that result in a single result (exit code and output). For example: date, date | awk '{print $1}' and date &amp;&amp; dateExpectations is a list of predictions in the form of rules that describe the output. For example: &quot;Output is exactly Hello World&quot; or &quot;Output start with foo&quot;Exit Code is the numeric code with which the shell expressions is expected to end (defaults to OK, aka 0)  The Parsing phase extracts all testcases from the provided test file(s).  ","version":"Next","tagName":"h3"},{"title":"Phase: Execution‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#phase-execution","content":" The shell expression of the testcase needs to be executed in order to decide whether the output matches expectations. The Executor is responsible to run a set of shell expressions. The StatefulExecutor is currently used for executing Markdown files, and the BashScriptExecutor to execute Cram files.  The execution phase results in one Output per testcase, that captures STDOUT, STDERR and the exit code.  ","version":"Next","tagName":"h3"},{"title":"Phase: Validation‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#phase-validation","content":" The output of execution for each testcase is checked against the expectations of the testcase. If the exit code mismatches, then the validation is immediately considered a failure and ends in an error.  If the exit code matches, then the output is compared line-by-line with the expectations by the DiffTool. If any comparison ends in the following states, then the whole validation is considered a failure:  Unmatched Expectation: An expectation does not match any output lineUnexpected Output: One or more lines of the output cannot be matched  ","version":"Next","tagName":"h3"},{"title":"Phase: Presentation‚Äã","type":1,"pageTitle":"Development","url":"/scrut/docs/advanced/development/#phase-presentation","content":" Lastly the the outcome of the previous validation is renderer it into either a human readable diff-like text or a machine interpretable interchange format (JSON or YAML). ","version":"Next","tagName":"h3"},{"title":"Expectations","type":0,"sectionRef":"#","url":"/scrut/docs/advanced/expectations/","content":"","keywords":"","version":"Next"},{"title":"Quantifiers‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#quantifiers","content":" The Quantifiers can be understood as following (nothing new if you are familiar with regular expressions):  ?: Zero or one occurrence; basically an optional output line*: Any amount of occurrences (0..n); no line, one line, more lines - all good+: One or more occurrences (1..n); at least one line, more are fine  Quantifiers can be used with most expectations, see the examples and description below for more details.  ","version":"Next","tagName":"h2"},{"title":"Equal Expectation‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#equal-expectation","content":" The Equal Expectation denotes a single line of output that ends in a newline character. Because this expectation is the most common one you do not need to provide the specific kind. Here an example:  A test ```scrut $ echo Hello Hello ```   The line that consists only of Hello is the Equal Expectation and specifies that the (first line of the) output must be equal to Hello\\n (with \\n being the newline of the operating system).  An extended for of the same Equal Expectation with explicit kind works as well and looks like that:  A test ```scrut $ echo Hello Hello (equal) ```   The explicit form makes most sense in conjunction with quantifiers:  A test ```scrut $ echo -e &quot;Hello\\nHello\\nHello&quot; Hello (equal+) ```   ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#examples","content":" Expression\tMeaningHello\tOne output line of the form Hello\\n Hello (equal)\tOne output line of the form Hello\\n Hello (?)\tOptional (zero or one) output line of the form Hello\\n Hello (*)\tAny amount (0..n) of output lines of the form Hello\\n Hello (+)\tOne or more (1..n) of output lines of the form Hello\\n Hello (equal*)\tAny amount (0..n) of output lines of the form Hello\\n Hello (equal+)\tOne or more (1..n) of output lines of the form Hello\\n  Note: You can use eq as a shorthand for equal  ","version":"Next","tagName":"h3"},{"title":"Equal No EOL Expectation‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#equal-no-eol-expectation","content":" Very close to the above, but much rarer, the Equal No EOL Expectation matches lines that do not end in a newline. Consider:  A test ```scrut $ echo -n Hello Hello (no-eol) ```   The above echo -n Hello prints Hello without a tailing newline character (there is no \\n at the end of Hello).  This Expectation could possibly only be the last line of output, so quantifiers make little sense.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#examples-1","content":" Expression\tMeaningHello (no-eol)\tOne output line of the form Hello - a line that does not end in newline  ","version":"Next","tagName":"h3"},{"title":"Glob Expectation‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#glob-expectation","content":" Glob Expectations are support two wildcard characters:  ? matches exactly one occurrence of any character* matches arbitrary many (including zero) occurrences of any character  Together with quantifiers, this allows for powerful if imprecise matches of output lines.  This will work ```scrut $ echo Hello You Hello* (glob) ``` This will work, too ```scrut $ echo -e &quot;Hello\\nHello There\\nHello World&quot; Hello* (glob+) ```   ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#examples-2","content":" Expression\tMeaningHello? (glob)\tA single output line that starts with Hello followed by one character Hello* (glob)\tA single output line that starts with Hello *Hello* (glob)\tA single output line that contains Hello *Hello (glob)\tA single output line that ends with Hello *Hello* (glob?)\tAn optional output line that contains Hello *Hello* (glob*)\tAny amount (0..n) of output lines that contain Hello *Hello* (glob+)\tOne or more (1..n) of output lines that contain Hello  Note: You can use gl as a shorthand for glob  ","version":"Next","tagName":"h3"},{"title":"Regex Expectation‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#regex-expectation","content":" Regular Expressions are the most powerful, yet precise, output describing rules that are supported. That comes at the price of complexity. Explaining regular expression syntax literarily fills books, so here is not the place to attempt that. Rust uses a RE2 inspired engine. Its syntax is very similar to it. It most notably differs from Perl's PCRE because it doesn't support backtracking to ensure good performance.  Nonetheless, an obligatory example:  This will work ```scrut $ echo Hello You Hello.+ (regex) ``` This will work, too: ```scrut $ echo -e &quot;Hello\\nEnding in Hello\\nHello Start&quot; .*Hello.* (regex+) ```   Note: All Regex Expectations are implicitly embedded within start and end markers: ^&lt;expression&gt;$. This means regular expressions are always assumed to match the full line. Use .* to explicitly match only at the end of (.*&lt;expression&gt; (regex)), or the start of (&lt;expression&gt;.* (regex)), or anywhere in (.*&lt;expression&gt;.* (regex)) a line.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#examples-3","content":" Expression\tMeaningHello.* (regex)\tA single output line that starts with Hello .*Hello.* (regex)\tA single output line that contains Hello .*Hello (regex)\tA single output line that ends with Hello .*Hello.* (regex?)\tAn optional output line that contains Hello .*Hello.* (regex*)\tAny amount (0..n) of output lines that contain Hello .*Hello.* (regex+)\tOne or more (1..n) of output lines that contain Hello Foo: [0-9]+ (regex+)\tOne or more (1..n) of output lines that start with Foo followed by a colon :, a whitespace and then only numbers till the end of the line  Note: You can use re as a shorthand for regex  ","version":"Next","tagName":"h3"},{"title":"Escaped Expectation‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#escaped-expectation","content":" CLIs usually only do (and mostly should) print out, well, printable characters. However, there are scenarios which you need to write binary data to STDOUT (e.g. consider a command line that generates a binary JPEG and pipes that output into yet another command that shrinks it or something $ create-jpeg | shrink-image). In addition to that adding colors can help make the output better readable - and some daredevils even throw in some emojis ü§¨. Lastly, consider the good old tab character \\t, which may be hard to read (or write) in a text editor.  Scrut tests live in Markdown or Cram files that are intended to be edited by users. They should not contain binary, non-printable data. To that end, any non-printable output can be denoted in it's hexadecimal escaped form \\xAB (with AB being the hexadecimal value of the bytecode of the character) or \\t to denote tab characters.  The following example shows an expectation of a string that renders as a bold, red font on the command line  Colorful fun ```scrut $ echo -e 'Foo \\033[1;31mBar\\033[0m Baz' Foo \\x1b[1mBar\\x1b[0m Baz (escaped) ```   Or consider some program that prints out two \\x00 separated strings:  Colorful fun ```scrut $ some-program foo\\x00bar (escaped) ```   Or again, the good old tab character:  Love the CSV ```scrut $ csv-generator foo\\tbar\\tbaz (escaped) ```   Note: Newlines are ignored for Escaped Expectations. So foo\\tbar (escaped) matches both foo\\tbar\\n and foo\\tbar.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#examples-4","content":" Expression\tMeaningHello\\tWorld (escaped)\tOne output line of that starts with Hello, followed by a tab character, followed by World Hello\\tWorld (escaped?)\tAn optional output line that contains Hello, followed by a tab character, followed by World Hello\\tWorld (escaped*)\tAny amount (0..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World Hello\\tWorld (escaped+)\tOne or more (1..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World  Note: You can use esc as a shorthand for escaped  ","version":"Next","tagName":"h3"},{"title":"Escaped Glob Expectations‚Äã","type":1,"pageTitle":"Expectations","url":"/scrut/docs/advanced/expectations/#escaped-glob-expectations","content":" Because it came up often enough, you can use (escaped) in combination with (glob):  Glob escaped output ```scrut $ csv-generator foo\\t* (escaped) (glob+) bar\\tbaz (escaped) ```   The above exports one or more lines of output that start with foo followed by tab. The last line of output is expected to be bar, followed by tab, followed by baz.  Expression\tMeaningHello\\tWorld* (escaped) (glob)\tOne output line of that starts with Hello, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob?)\tAn optional output line that contains Hello, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob*)\tAny amount (0..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob+)\tOne or more (1..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World, followed by anything  Note: You can use shorthands for either. Quantifiers must be always on glob. ","version":"Next","tagName":"h3"},{"title":"File Formats","type":0,"sectionRef":"#","url":"/scrut/docs/advanced/file-formats/","content":"","keywords":"","version":"Next"},{"title":"File Anatomy‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#file-anatomy","content":" All test files contain one or more test cases. There are two common patterns to structure test files in Scrut:  Coherent Test Suite (recommended): One test file represents one use-case or behavior. This makes it easy to identify broken functionality.List of Tests: One test file contains a list of simple, not necessarily related tests.  Markdown files support document wide configuration in the form of &quot;YAML Frontmatter&quot;.  ","version":"Next","tagName":"h2"},{"title":"Test Case Anatomy‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#test-case-anatomy","content":" Each individual test that lives in a test file is called a Test Case and consists of the following components:  A Title, so that a human can understand what is being doneA Shell Expression, that can be anything from a single command to a multi-line, multi-piped expressionExpectations of the output that the Shell Expression will yieldOptionally the expected Exit Code the Shell Expression must end in - if anything but successful execution (0) is expectedOptionally per-test-case configuration (only supported by Markdown format)  ","version":"Next","tagName":"h3"},{"title":"Markdown Format‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#markdown-format","content":" Markdown is an amazingly simple, yet powerful language. To write Test Cases in Markdown follow this guidance:  Shell Expressions and Expectations live in the same code-block, that must be annotated with the language scrut The first line of a Shell Expressions must start with $ (dollar, sign followed by a space), any subsequent with &gt; (closing angle bracket / chevron, followed by a space)All other lines in the code block (including empty ones) that follow the Shell Expression are considered ExpectationsLines starting with # that precede the shell expression are ignored (comments)If an Exit Code other than 0 is expected, it can be denoted in square brackets [123] once per Test Case The first line before the code block that is either a paragraph or a header will be used as the Title of the Test Case  Here an example:  This is the title ```scrut $ command | \\ &gt; other-command expected output line another expected output line [123] ```   The following constraints apply:  A markdown file can contain as many Test Cases as needed (1..n)Each code block in a Test Case may only have one (1) Shell Expression (each Test Case is considered atomic)Code blocks that do not denote a language (or a different language than scrut) will be ignored  With that in mind, consider the following markdown file that contains not only Test Cases but arbitrary other text and other code blocks. This is idiomatic Scrut markdown files that combines tests and documentation:  # This is just regular markdown It contains both Scrut tests **and** abitrary text, including code examples, that are unrelated to Scrut. ```python import os print(&quot;This code block ignored by Scrut&quot;) ``` ## Here is a scrut test ```scrut $ echo Hello Hello ``` ## Embedded with other documentation So it's a mix of test and not tests. Any amount of tests are fine: ```scrut $ echo World World ``` Just make sure to write only one Test Case per code-block.   Note: If you are testing actual markdown output, be aware that you can embed code blocks in other code blocks, if the outer code block uses one more backtick (opening and closing!) than the embedded one(s). Just have a look at the source code of this file right above this text.  ","version":"Next","tagName":"h2"},{"title":"Inline Configuration‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#inline-configuration","content":" Scrut supports two kinds of inline configuration:  Per Document (document-wide) configuration, which can be defined at the start of the test filePer Test Case (test-case-wide) configuration, which can be defined with each individual Test Case  Example  --- # document-wide YAML configuration total_timeout: 30s --- # The test document The initial block that is initialized with `---` and terminated with `---` contains the configuration in YAML notation. ## A simple test ```scrut $ echo Hello One Hello One ``` The above test does not contain any per-test configuration ## A test with configuration ```scrut {timeout: 10s} $ echo Hello Two Hello Two ``` The above test contains per-test configuration   Some inline-configuration attribute can overwritten by parameters provided on the command-line. The order of precedence is:  Command-line parameterPer-TestCase configurationPer-Document configurationDefault  Document Configuration‚Äã  Name\tType\tCorresponding Command Line Parameter\tDescriptionappend\tlist of strings\t--append-test-file-paths\tInclude these paths in order, as if they were part of this file. All tests within the appended paths are appended to the tests defined in this file. Use-case is common/shared test tear-down. Paths must be relative to the current $TESTDIR. defaults\tTestCase Configuration\tn/a\tDefaults for per-test-case configuration within the test file. prepend\tlist of strings\t--prepend-test-file-paths\tInclude these paths in order, as if they were part of this file. All tests within the prepend paths are prepended to the tests defined in this file. Use-case is common/shared test setup. Paths must be relative to the current $TESTDIR. shell\tstring\t--shell\tThe path to the shell. If a full path is not provided, then the command must be in $PATH. Only bash compatible shells are currently supported! total_timeout\tduration string\t--timeout-seconds\tAll tests within the file (including appended and prepended) must finish executing within this time.  Defaults (Markdown and Cram)  append: [] defaults: {} prepend: [] shell: bash total_timeout: 15m   Caveats  Per-document configuration in files that are appended or prepended is ignored  TestCase Configuration‚Äã  Name\tType\tCorresponding Command Line Parameter\tDescriptiondetached\tboolean\tn/a\tTell Scrut that the shell expression of this test will detach itself, so Scrut will not consider this a test (i.e. no output or exit code evaluation). Purpose is to allow the user to detach a command (like nohup some-command &amp;) that is doing something asynchronous (e.g. starting a server to which the tested CLI is a client). environment\tobject\tn/a\tA set of environment variable names and values that will be explicitly set for the test. keep_crlf\tboolean\t--keep-output-crlf\tWhether CRLF should be translated to LF (=false) or whether CR needs to be explicitly handled (=true). output_stream\tenum (stdout, stderr, combined)\t--combine-output and --no-combine-output\tWhich output stream to choose when applying output expectations: stdout (all expectations apply to what is printed on STDOUT), stderr (all expectations apply to what is printed on STDERR), combined (STDOUT and STDERR will combined into a single stream where all expectations are applied on) skip_document_code\tpositive integer\tn/a\tThe exit code, that if returned by any test, leads to skipping of the whole file. timeout\tnull or duration string\tn/a\tA max execution time a test can run before it is considered failed (and will be aborted). wait\tnull or duration string or Wait Configuration\tn/a\tSee Wait Configuration  Defaults (Markdown)  detached: false environment: {} keep_crlf: false output_stream: stdout skip_document_code: 80 timeout: null wait: null   Defaults (Cram)  detached: false environment: {} keep_crlf: true output_stream: combined skip_document_code: 80 timeout: null wait: null   ","version":"Next","tagName":"h3"},{"title":"Wait Configuration‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#wait-configuration","content":" This configuration corresponds to the per-test-case detached configuration and helps to write client / server tests where first a server is started (i.e. a test that runs detached) and then a client communicates with the server (i.e. a test that waits)  Name\tType\tDescriptiontimeout\tduration string\tHow long to wait for the test to run. path\tnull or string\tIf set then the wait will end early once the path exists. This path must be in $TMPDIR  Example  # A server/client test example Show-case how a server/client test that initially starts a server ## Start a server ```scrut {detached: true} $ my-server --start &amp;&amp; touch &quot;$TMPDIR&quot;/server-started ``` ## Run client test once server is up ```scrut {wait: {timeout: 5m, path: server-started}} $ my-client --do-a-thing ```   ","version":"Next","tagName":"h3"},{"title":"Cram Format‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#cram-format","content":" Also supported, for compatibility, is the Cram file format. The general guidance to write Test Cases in Cram files is:  The first line of Shell Expression must start with $ (space + space + dollar + space), any subsequent with &gt; (space + space + closing angle bracket + space) This is slightly different from classic scrut syntax. Be mindful of the additional spaces Lines following the Shell Expression, that are also indented with two spaces, are considered Expectations If an Exit Code other than 0 is expected, it can be denoted in square brackets [123] once per Test CaseNote: Empty output lines (=empty Expectations) must still have two leading space charactersNote: A fully empty line (no leading spaces) denotes the end of the current Test Case If the Shell Expression is preceded by a non-empty line (that is not indented) the line is considered the Title of the Test Case  Here an example:  This is a comment $ scrut --help Scrut help output Another Test Case in the same file $ scrut --version Scrut version output   Multiple tests Test Cases can be written in sequence, without any empty lines in between:  A title for the first Test Case $ first --command $ second --command $ third --comand Output Expectation   Note: Remember the indenting space characters!  ","version":"Next","tagName":"h2"},{"title":"Which format to chose?‚Äã","type":1,"pageTitle":"File Formats","url":"/scrut/docs/advanced/file-formats/#which-format-to-chose","content":" We recommend the Markdown format which was introduced with two goals in mind:  Tests ‚ù§Ô∏è Documentation: The value of tests is not only in proving behavior, but also in documenting it - and thereby also in teaching it. The Markdown Test Case format allows you to keep tests around in a way that future generations of maintainers will love you for.Bad Spaces üëæ: To denote an expected empty line of output in Cram format you have to provide two empty spaces . This goes counter a lot of default behavior in the development toolchain. Many CI/CD tools are tuned to automatically ignore changes that only pertain spaces. Code review tools often deliberately hide those changes. Spaces are generally hard to see in code editors - if they are visualized at all. Breaking tests that are caused by an accidentally removed or added space cause rage quitting.  If these arguments resonate with you, go for the Markdown format. If not you are probably better of with Cram that allows for a more condensed writing style. Choices, choices. ","version":"Next","tagName":"h2"},{"title":"Specifics","type":0,"sectionRef":"#","url":"/scrut/docs/advanced/specifics/","content":"","keywords":"","version":"Next"},{"title":"Test output‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#test-output","content":" Executing a test with Scrut results either in success (when all expectations in the test match) or failure (when at least one expectation in the test does not match).  Scrut supports multiple output renderers, which yield a different representation of the test results.  ","version":"Next","tagName":"h2"},{"title":"Pretty Renderer (default)‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#pretty-renderer-default","content":" Scrut will always tell you what it did:  $ scrut test selftest/cases/regex.md Result: 1 file(s) with 8 test(s): 8 succeeded, 0 failed and 0 skipped   In case of failure the pretty default renderer will provide a human-readable output that points you to the problem with the output:  $ scrut test a-failing-test.md // ============================================================================= // @ /path/to/a-failing-test.md:10 // ----------------------------------------------------------------------------- // # One conjunct expression // ----------------------------------------------------------------------------- // $ echo Foo &amp;&amp; \\ // echo Bar // ============================================================================= 1 1 | Foo 2 | - BAR 2 | + Bar 3 | + Baz   The failure output consists of two components:  The failure header, which consists of all initial lines that start with //, indicates the positionThe failure body, which consists of all the following lines, indicates the problem  Header  The header contains three relevant information. Given the above output:  @ /path/to/a-failing-test.md:4, tells you that the test that failed is in the provided file /path/to/a-failing-test.md and that the shell expression (that failed the test) starts in line four of that file.# &lt;test title&gt;, gives you the optional title of the test in the file. See File Formats) to learn more. If the test does not have a title, this line is omitted.$ &lt;test command&gt;, is the shell expectation from the test file that is tested and that has failed. Again, see File Formats) for more information.  Body  There are two possible variants that the diff renderer may return:  Failed output expectationsFailed exit code expectation  The above output is a failed output expectations and you can read it as following:  1 1 | Foo: This line was printed as expected. The left hand 1 is the number of the output line and the right hand 1 is the number of the expectation. 2 | - BAR: This line was expected, but not printed. The left hand omitted number indicates that it was not found in output. The right hand number tells that this is the second expectation. The - before the line Bar emphasizes that this is a missed expectation.2 | + Bar: This line was printed and expected. The left hand 2 is the number of the output line and the right hand 3 is the number of the expectation.3 | + Baz: This line was printed unexpectedly. The left hand 3 is the number of the output line the omitted right hand number implies there is no expectation that covers it. The + before the line Zoing emphasizes that this is a &quot;surplus&quot; line.  Note: If you work with test files that contain a large amount of tests, then you may want to use the --absolute-line-numbers flag on the command line: instead of printing the relative line number for each test, as described above, it prints absolute line numbers from within the test file. Assuming the Foo expectation from above is in line 10 of a file, it would read 13 13 | Foo - and all subsequent output liens with respective aligned line numbers.  An example for the body of an exit code expectation:  unexpected exit code expected: 2 actual: 0 ## STDOUT #&gt; Foo ## STDERR   This should be mostly self-explanatory. Scrut does not provide any output expectation failures, because it assumes that when the exit code is different, then it is highly likely that the output is very different - and even if not, it would not matter, as it failed anyway.  The tailing ## STDOUT and ## STDERR contain the output lines (prefixed with #&gt; ) that were printed out from the failed execution.  ","version":"Next","tagName":"h3"},{"title":"Diff renderer‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#diff-renderer","content":" The diff renderer, that can be enabled with --renderer diff (or -r diff), prints a diff in the unified format.  $ scrut test -r diff a-failing-test.md --- /path/to/a-failing-test.md +++ /path/to/a-failing-test.md.new @@ -14 +14,2 @@ malformed output: One conjunct expression -BAR +Bar +Baz   Note: The created diff is compatible with the patch command line tool (e.g. patch -p0 &lt; &lt;(scrut test -r diff a-failing-test.md)).  ","version":"Next","tagName":"h3"},{"title":"JSON and YAML renderer‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#json-and-yaml-renderer","content":" These renderer are primarily intended for automation and are to be considererd experimental. You can explore them using --renderer yaml or respective --renderer json.  ","version":"Next","tagName":"h3"},{"title":"Test environment variables‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#test-environment-variables","content":" Scrut sets a list of environment variables for the execution. These are set in addition to and overwriting any environment variables that are set when scrut is being executed.  Note: If you need an empty environment, consider executing using env, like env -i scrut test .. instead  ","version":"Next","tagName":"h2"},{"title":"Scrut specific environment variables‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#scrut-specific-environment-variables","content":" TESTDIR: contains the absolute path of the directory where the file that contains the test that is currently being executed is inTESTFILE: contains the name of the file that contains the test that is currently being executedTESTSHELL: contains the shell that in which the test is being executed in (default /bin/bash, see --shell flag on commands)TMPDIR: contains the absolute path to a temporary directory that will be cleaned up after the test is executed. This directory is shared in between all executed tests across all test files.  ","version":"Next","tagName":"h3"},{"title":"Common (linux) environment variables‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#common-linux-environment-variables","content":" CDPATH: emptyCOLUMNS: 80GREP_OPTIONS: emptyLANG: CLANGUAGE: CLC_ALL: CSHELL: Same as TESTSHELL, see aboveTZ: GMT  ","version":"Next","tagName":"h3"},{"title":"(Optional) Cram environment variables‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#optional-cram-environment-variables","content":" When using the --cram-compat flag, or when a Cram .t test file is being executed, the following additional environment variables will be exposed for compatibility:  CRAMTMP: if no specific work directory was provided (default), then it contains the absolute path to the temporary directory in which per-test-file directories will be created in which those test files are then executed in (CRAMTMP=$(realpath &quot;$(pwd)/..&quot;)); otherwise the path to the provided work directoryTMP: same as TMPDIRTEMP: same as TMPDIR  ","version":"Next","tagName":"h3"},{"title":"Test work directory‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#test-work-directory","content":" By default scrut executes all tests in a dedicated directory per test file. This means all tests within one file are being executed in the same directory. The directory is created within the system temporary directory. It will be removed (including all the files or directories that the tests may have created) after all tests in the file are executed - or if the execution of the file fails for any reason.  This means something like the following can be safely done and will be cleaned up by Scrut after the test finished (however it finishes):  # Some test that creates a file ```scrut $ date &gt; file ``` The `file` lives in the current directory ```scrut $ test -f &quot;$(pwd)/file&quot; ```   The directory within which tests are being executed can be explicitly set using the --work-directory parameter for the test and update commands. If that parameter is set then all tests from all test files are executed run within that directory, and the directory is not removed afterwards.  Note: In addition to the work directory Scrut also creates and cleans up a temporary directory, that is accessible via $TMPDIR. Tools like mktemp automatically use it (from said environment variable).  ","version":"Next","tagName":"h2"},{"title":"Test execution‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#test-execution","content":" As Scrut is primarily intended as an integration testing framework for CLI applications, it is tightly integrated with the shell. Each Scrut test must define a shell expression (called an &quot;execution&quot;). Each of those executions is then run within an actual shell (bash) process, as they would be when a human or automation would execute the expression manually on the shell.  With that in mind:  Each execution from the same test file is executed in an individual shell process. Scrut currently only supports bash as shell process.Each subsequent execution within the same file inherits the state of the previous execution: environment variables, shell variables, functions, settings (set and shopt). Tests within the same file are executed in sequential order.Executions happen in a temporary work directory, that is initially empty and will be cleaned up after the last executions of the test file has run (or when executions are skipped).Executions may be detached, but Scrut will not clean up (kill) or wait for detached child processes If you want to run your process in the background or detach, see the detached setting in the testcase configuration page.  ","version":"Next","tagName":"h2"},{"title":"Execution within a custom shell‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#execution-within-a-custom-shell","content":" While Scrut currently only supports bash (&gt;= 3.2) a custom shell can be provided with the --shell command line parameter. To understand how that works consider the following:  $ echo &quot;echo Hello&quot; | /bin/bash - Hello   What the above does is piping the string echo Hello into the STDIN of the process that was started with /bin/bash -. Scrut pretty much does the same with each shell expressions within a test file.  So why provide a custom --shell then? This becomes useful in two scenarios:  You need to execute the same code before Scrut runs each individual expressionYou need Scrut to execute each expression in some isolated environment  For (1) consider the following code:  #!/bin/bash # do something in this wrapper script source /my/custom/setup.sh run_my_custom_setup # consume and run STDIN source /dev/stdin   For (2) consider the following:  #!/bin/bash # do something in this wrapper script source /my/custom/setup.sh run_my_custom_setup # end in a bash process that will receive STDIN exec ssh username@acme.tld /bin/bash   Instead of SSHing into a machine, consider also running a bash process in docker container.  ","version":"Next","tagName":"h3"},{"title":"STDOUT and STDERR‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#stdout-and-stderr","content":" Commands-line applications can generate output on to two streams: STDOUT and STDERR. There is no general agreement on which stream is supposed to contain what kind of data, but commonly STDOUT contains the primary output and STDERR contains logs, debug messages, etc. This is also the recommendation of the CLI guidelines.  Scrut validates CLI output via Expectations. Which output that entails can be configured via the output_stream configuration directive (and the --(no-)combine-output command-line parameters).  Note: While you can configure which output streams Scrut considers when evaluating output expecations, you can also steer this by using stream control bash primitives like some-command 2&gt;&amp;1.  ","version":"Next","tagName":"h2"},{"title":"Exit Codes‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#exit-codes","content":" You can denote the expected exit code of a shell expression in a testcase. For example:  The command is expected to end with exit code 2 ```scrut $ some-command --foo an expected line of output [2] ```   Unless otherwise specified an exit code of 0 (zero) is assumed. You can explicitly denote it with [0] if you prefer.  Note: Exit code evaluation happens before output expectations are evaluated.  ","version":"Next","tagName":"h2"},{"title":"Skip Tests with Exit Code 80‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#skip-tests-with-exit-code-80","content":" If any testcase in a test file exist with exit code 80, then all testcases in that file are skipped.  This is especially helpful for OS specific tests etc. Imagine:  Run tests in this file only on Mac ```scrut $ [[ &quot;$(uname)&quot; == &quot;Darwin&quot; ]] || exit 80 ```   Note: The code that Scrut accepts to skip a whole file can be modified with the skip_document_code configuration directive.  ","version":"Next","tagName":"h3"},{"title":"Scrut Exit Code‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#scrut-exit-code","content":" Scrut itself communicates the outcome of executions with exit codes. Currently three possible exit codes are supported:  0: Command succeeded, all is good (scrut test, scrut create, scrut update)1: Command failed with error (scrut test, scrut create, scrut update)50: Validation failed (scrut test only)  ","version":"Next","tagName":"h3"},{"title":"Newline handling‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#newline-handling","content":" Newline endings is a sad story in computer history. In Unix / MacOS ( / *BSD / Amiga / ..) the standard line ending is the line feed (LF) character \\n. Windows (also Palm OS and OS/2?) infamously attempted to make a combination of carriage return (CR) and line feed the standard: CRLF (\\r\\n). Everybody got mad and still is.  See the keep_crlf configuration directive to understand how Scrut handles LF and CRLF and how you can modify the default behavior.  ","version":"Next","tagName":"h2"},{"title":"Execution Environment‚Äã","type":1,"pageTitle":"Specifics","url":"/scrut/docs/advanced/specifics/#execution-environment","content":" A Scrut test file can contain arbitrary amounts of tests. Scrut provides a shared execution environment for all tests within a single file, which comes with certain behaviors and side-effects that should be known:  Shared Shell Environment: Each subsequent testcase in the same file inherits the shell environment of the previous testcase. This means: All environment variables, shell variables, aliases, functions, etc that have are set in test are available to the immediate following test. Exception: Environments from detached testcases are not passed along Shared Ephemeral Directories: Each testcase in the same test file executes in the the same work directory and is provided with the same temporary directory ($TEMPDIR). Both directories will be removed (cleaned up) after test execution - independent of whether the test execution succeeds or fails. Exception: If the --work-directory command-line parameter is provided, then this directory will not be cleaned up (deleted) after execution. A temporary directory, that will be removed after execution, will be created within the working directory. Process Isolation: Scrut starts individual bash processes for executing each testcase of the same test file. Each shell expression. The environment of the previous execution is pulled in through a shared state file, that contains all environment variables, shell variables, aliases, functions and settings as they were set when the the previous testcase execution ended. Exception: All testcases in cram files are currently executed within the same bash process - this is likely to change in the future. ","version":"Next","tagName":"h2"},{"title":"Tutorial","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#prerequisites","content":" To make it very simple to follow along, this guide uses the modern, but well established jq command line tool as the CLI that is tested in all provided code examples. Deep understanding of jq is not required, but it would help if you have at least some grasp what it does and how to use it. If that is not the case, yet: it is a truly, amazingly useful tool; now is a great time to learn about!  The following should work on your terminal:  # scrut itself should be installed $ scrut --version scrut 0.2.0 # jq should be installed $ jq --version jq-1.6   Note: In all shell code blocks within this document lines prefixed with $ are commands, lines prefixed with # are comments and any other line can be assumed to be the output of the previous command  ","version":"Next","tagName":"h2"},{"title":"About file structure‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#about-file-structure","content":" Scrut does not require any particular file structure. This tutorial is assuming that the files would be stored in a integration-tests subdirectory together with the source-code of the CLI that is tested.  # going to the directory that contains the source code $ cd ~/Projects/jq # creating a new directory that is going to contain the tests $ mkdir integration-tests   Although Scrut has no requirements towards file structure it is recommended, that all test relating files (see more below) are in the same directory as the test files themselves, which makes referencing them easier.  ","version":"Next","tagName":"h3"},{"title":"Decide what to test first‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#decide-what-to-test-first","content":" What then is the first thing to test about our CLI jq? What is the first thing to test about any CLI? Maybe you have a great answer that fits perfectly for your specific CLI. If you don't then consider to start with a smoke test: When I switch it on, do I see smoke rising up?  Translated to a CLI that means: executing the tool in the most basic way possible, does it panic / fatal / die unexpectedly? Considering you keep developing your CLI, such a basic test answers the question: Did you break something very fundamental?  And what would be a good smoke test for a CLI? For jq it is the execution from above (jq --version) seems like a great candidate. For other CLIs it might be --help instead. Either way, you want to choose something that doesn't have much complexity, that doesn't rely on any external dependencies. If you are the author of the CLI that should be easy to find.  ","version":"Next","tagName":"h2"},{"title":"Pattern: Automatic Test Creation‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-automatic-test-creation","content":" Finally, let's get to writing the test. Actually writing seems too bothersome. Sure, you could, but how about you generate it instead? Do that:  $ scrut create --output integration-tests/smoke.md -- jq --version Writing generated test to `integration-tests/smoke.md`   Ok, let me unpack that for you:  scrut create - tells Scrut to execute a command and create a test from its output--output integration-tests/smoke.md - lets scrut know where to write the created test to-- - signifies the end of options for scrut; all that follows is part of the command for which a test is generatedjq --version - that is the command (the Shell Expression) which scrut is going to execute and from which's output it is going to generate test Expectations  This also could have been written differently:  $ echo &quot;jq --version&quot; | scrut create - &gt; integration-tests/smoke.md Writing generated test to STDOUT   Here the string jq --version was piped to the STDIN of scrut create (which was made aware of that by having one argument -) and the output (to STDOUT) was delegated into the same output file as before.  Both are valid forms and result in the same outcome, that is a new test in the file integration-tests/smoke.md. The contents of that file should be like that (aside from the version string, that is likely different for you):  # Command executes successfully ```scrut $ jq --version jq-1.6 ```   While you are looking at it, how about you change that title to Smoke test or something like that. Half of the value of a Scrut test file is the documentation, so it is always worth to put in some time to clarify intentions and describe expectations.  Don't touch the rest - for now. We'll get to that in a minute. You can read up on the anatomy of the file, here a very quick primer:  Scrut test files are markdown documentsCode blocks of language scrut contain the tested commands and the expected output  ","version":"Next","tagName":"h2"},{"title":"Run the first Test‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#run-the-first-test","content":" Running tests is the bread and butter of Scrut. It is - literally - what it is made for. So without further ado:  $ scrut test integration-tests/smoke.md Validation succeeded   Nice! That works. As it should be, since Scrut create the test for you. Although that was a bit anticlimactic. Let's make it more fun and go break it ü§°. Change the contents of the file like so:  # Smoke test ```scrut $ jq --version foo ```   Now run it again:  $ scrut test integration-tests/smoke.md // ============================================================================= // @ integration-tests/smoke.md // ----------------------------------------------------------------------------- // # Smoke test // ----------------------------------------------------------------------------- // $ jq --version // ============================================================================= 1 | - foo 1 | + jq-1.6   Ok, it is getting interesting. What you are seeing here (likely in color) is an output validation error. The output expectations in the test file do not match with the output the command actually spits out. This is how you read it:  @ integration-tests/smoke.md: Location of the test file# Smoke test: Title of the test in the file$ task --version: Shell expression that resulted in invalid output 1 | - foo 1 | + jq-1.6 The first line 1 | - foo denotes that foo was expected from the test, but is missing in the output. The next line 1 | + jq-1.6 denotes that jq-1.6 was printed out as 1st line from the command, but is missing in the test.  ","version":"Next","tagName":"h3"},{"title":"Pattern: Resilient Tests‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-resilient-tests","content":" This is actually a good point in time to speak about brittle tests. Having the version (here jq-1.6) in the smoke.md file is not a good idea. Why? Because it is likely to change, because you keep developing it. Or someone is. Having that string in the test file will just create the worst kind of all work down the line: toil.  Also consider: Does having the version in there really provide value? The idea of the smoke test is to fail if things are so broken, that basically nothing works anymore. From that perspective, there is no need to check about the version: let's get rid of this nascent technical debt.  So how do you do that? Well, how would you do it on the shell? You would do something like that:  $ jq --version &gt; /dev/null   And that is exactly how you would do it in the test:  # Smoke test ```scrut $ jq --version &gt; /dev/null ```   Is that still a meaningful test? Yes, it is! It still tests whether the command executes successfully. What does successfully mean? Well, whether it exits with a 0 exit code. That is an implicit test any test case will automatically provide. Don't take my word for it, though. Change the expected exit code to, say, 10 and see what happens. Just add a new line containing [10] after the shell expression:  # Smoke test ```scrut $ jq --version &gt; /dev/null [10] ```   Now test it:  $ scrut test integration-tests/smoke.md // ============================================================================= // @ integration-tests/smoke.md // ----------------------------------------------------------------------------- // # Smoke test // ----------------------------------------------------------------------------- // $ jq --version &gt; /dev/null // ============================================================================= unexpected exit code expected: 10 actual: 0 ## STDOUT ## STDERR   As promised: it fails. The output should be self explanatory. Read more about exit codes here.  Going forward remove the [10] again, so that the test is in a working state.  ","version":"Next","tagName":"h2"},{"title":"Pattern: Test Fixtures‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-test-fixtures","content":" Ok, let's start with testing actual functionality. No worries, we won't attempt to cover all that jq can do with tests in this tutorial. Just enough to show some good to know patterns. Here is one, if a bit obvious: a good idea to start with any test is executing it on the shell.  Since jq is a neat tool to manipulate JSON, we need some JSON to manipulate. Let's use the same as the jq tutorial itself, that is the Github history of the jq repository:  $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' # not gonna show the output, it is a lot   Let's say we want to write a test that proves and documents the (imho) core functionality of jq: mutating JSON. As an example we are going to reduce those huge JSON dumps into something more manageable: who's commit was committed when. Each result item should have the following form: {&quot;who&quot;: &quot;&lt;name&gt;&quot;, &quot;when&quot;: &quot;&lt;date&gt;&quot;} . This is how you can achieve that on the the command line (names changed):  $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ jq '[.[] | {who: .commit.author.name, when: .commit.committer.date}]' [ { &quot;who&quot;: &quot;Person Name&quot;, &quot;when&quot;: &quot;2022-05-26T21:04:32Z&quot; }, { &quot;who&quot;: &quot;Another Person&quot;, &quot;when&quot;: &quot;2022-05-26T21:02:50Z&quot; }, { &quot;who&quot;: &quot;Even More&quot;, &quot;when&quot;: &quot;2022-05-26T21:02:10Z&quot; }, { &quot;who&quot;: &quot;And so forth&quot;, &quot;when&quot;: &quot;2022-05-26T21:01:25Z&quot; }, { &quot;who&quot;: &quot;Name Name&quot;, &quot;when&quot;: &quot;2022-05-26T20:53:59Z&quot; } ]   Ok, that shows that the transformation of the output works as we assumed it would. However, you probably have noted, using the curl output in the a test will not be very resilient, as the output is prone to change.  Since we are not really interested in the functionality of curl or Github (and quite frankly could without network dependencies), let's instead store the current output of the curl execution into a test fixture file in our integration-tests folder. This way we have a consistent input to run our test on:  $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' &gt; integration-tests/commits.json   Now we can start with writing the actual test file. Instead of using scrut create, start with the following template in integration-tests/transform-input.md:  # Transform input ```scrut $ cat &quot;$TESTDIR/commits.json&quot; | \\ &gt; jq '[.[] | {who: .commit.author.name, when: .commit.committer.date}]' [ { &quot;who&quot;: &quot;Person Name&quot;, &quot;when&quot;: &quot;2022-05-26T21:04:32Z&quot; }, { &quot;who&quot;: &quot;Another Person&quot;, &quot;when&quot;: &quot;2022-05-26T21:02:50Z&quot; }, { &quot;who&quot;: &quot;Even More&quot;, &quot;when&quot;: &quot;2022-05-26T21:02:10Z&quot; }, { &quot;who&quot;: &quot;And so forth&quot;, &quot;when&quot;: &quot;2022-05-26T21:01:25Z&quot; }, { &quot;who&quot;: &quot;Name Name&quot;, &quot;when&quot;: &quot;2022-05-26T20:53:59Z&quot; } ] ```   Note: The second (and any subsequent) line of a command starts with a &gt; character - unlike the first, which starts with a $ (read more). The tailing \\\\ in the first command line is needed, because /bin/bash needs it (both lines, stripped by their starting $ or &gt; character, are ultimately passed to the shell process, hence must comply with it's requirements).  ","version":"Next","tagName":"h2"},{"title":"Tests directory isolation‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#tests-directory-isolation","content":" You may have noted the that the commits.json file is referred to as &quot;$TESTDIR/commits.json&quot;. The reason for that is that each test is executed from within an empty test directory. The absolute path to the directory, where the actual test file is in is available via the $TESTDIR environment variable. Since commits.json is located in the same directory as transform-input.md the expression &quot;$TESTDIR/commits.json&quot; contains the absolute path to the commits.json file (read more).  ","version":"Next","tagName":"h3"},{"title":"Pattern: Test Bootstrapping‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-test-bootstrapping","content":" There is one more thing that should be done to make the test resilient: jq has a couple of command line parameters that decide how the output is being rendered. There are two in particular, which should be set in our case:  -r (raw output): Pertains to non-JSON output, in which strings would be quoted without it (let's not - easier to pipe into other command line programs)-M (monochrome, not colored output): While that is currently the default, it may change which would break our test-S (sort keys of objects): Currently, the keys are outputted as we provided them - but to be safe (have a resilient test), lets just explicitly sort them, then there is no question in their order  Using both of those keys would change the command in the jq &lt;..&gt; command in the test to jq -r -M -S &lt;..&gt;.  Thinking ahead, we are going to use these flags in every test, for the same reason why we are using it here (be very sure about the expected output). With that in mind, consider the following bash script:  #/bin/bash # tell bash exporting aliases is fine shopt -s expand_aliases # alias `jq`, so that it always executes with the two parameters alias jq='jq -r -M -S'   Store the above file under integration-tests/setup.sh, and then we can make use of it in our test file:  # Test transformation Test whether `jq` transforms tests as we ## Bootstrap ``` $ source &quot;$TESTDIR/setup.sh&quot; ``` ## Transform input ```scrut $ cat &quot;$TESTDIR&quot;/commits.json | \\ &gt; jq '[.[] | {who: .commit.author.name, when: .commit.committer.date}]' [ { &quot;when&quot;: &quot;2022-05-26T21:04:32Z&quot;, &quot;who&quot;: &quot;Person Name&quot; }, { &quot;when&quot;: &quot;2022-05-26T21:02:50Z&quot;, &quot;who&quot;: &quot;Another Person&quot; }, { &quot;when&quot;: &quot;2022-05-26T21:02:10Z&quot;, &quot;who&quot;: &quot;Even More&quot; }, { &quot;when&quot;: &quot;2022-05-26T21:01:25Z&quot;, &quot;who&quot;: &quot;And so forth&quot; }, { &quot;when&quot;: &quot;2022-05-26T20:53:59Z&quot;, &quot;who&quot;: &quot;Name Name&quot; } ] ```   Note: The order of who and when changed due to -S.  As you can see there are now two code blocks of the type scrut in the same file. That means there are two tests in that one file. This is fine, you can have as many test as make sense to you in a file. Scrut executes them in order, which allows the alias jq=.. set in setup.sh to affect the jq execution in the test file.  Bootstrapping tests is a very common strategy in Scrut and is considered idiomatic.  ","version":"Next","tagName":"h2"},{"title":"Bootstrapping, sounds familiar?‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#bootstrapping-sounds-familiar","content":" If you are familiar with unit testing (in whatever language), you likely came across the test suite pattern. If not, then in (very) short: A test suite is a semantic cohesive collection of tests, which is often run against different implementations of the same interface. Imagine a storage backend interface, for which an implementation LocalStorage writes on a local disk and RemoteStorage writes somewhere in the cloud. Both implement the same Storage interface and therefore can be tested by the same test suite StorageTestSuite.  In those scenarios it is not uncommon that each test-suite run executes specific &quot;setup code&quot; for each implementation, before all the tests are executed. You may often find methods named like setupTests, beforeTests or something akin.  A variant of bootstrapping is seeding where a specific methods are executed once before each test (as oppose to: once before all tests). Although the terms may be also be used interchangeable (depends on the language of the testing framework and developer's choice). Translated to Scrut you could have seed-some-state.sh files, that are then included in one or multiple tests, to keep the tests themselves clean and the code d.r.y.  ","version":"Next","tagName":"h3"},{"title":"Pattern: Update as a Workflow‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-update-as-a-workflow","content":" In the previous section quite a lot of copying from the terminal into text files happened. A tad bothersome and smells like a bad tedious process. Indeed. There is a better way.  Let's start with a new test. jq has a lot of built-in functions, so there is plenty to pick from. Since we were already interested in that committer date earlier, lets write a test for the fromdate function. Start with the following template, which is basically a copy of the previous test, but with the new command we want and with all outputs striped:  # Test built-in `fromdate` Assure the `fromdate` function parses ISO 8601 dates into unix timestamps ## Bootstrap ```scrut $ source &quot;$TESTDIR/setup.sh&quot; ``` ## Use `fromdate` ```scrut $ cat &quot;$TESTDIR&quot;/commits.json | \\ &gt; jq '.[] | .commit.committer.date | fromdate' ```   Having clear intentions in the leading markdown of a test file is a good practice. Here it makes it clear that we are expecting the output of some unix timestamps. Since we don't have any, it is to be expected that the test execution will fail. Only one way to be sure:  $ scrut test integration-tests/builtin-fromdate.md // ============================================================================= // @ integration-tests/builtin-fromdate.md // ----------------------------------------------------------------------------- // # Use `fromdate` // ----------------------------------------------------------------------------- // $ cat &quot;$TESTDIR&quot;/commits.json | \\ // jq '.[] | .commit.committer.date | fromdate' // ============================================================================= 1 | + 1653599072 2 | + 1653598970 3 | + 1653598930 4 | + 1653598885 5 | + 1653598439   This output tells us two things:  It seems fromdate can parse our dates and transform them into unix timestampsThe test fails, because it does not mention the expected output  At least the latter is not completely surprising. In order to make the test green, we could again copy the output into the test. However, there is a better way - as promised:  $ scrut update --replace integration-tests/builtin-fromdate.md   This shows you the same failed test output again. However, in addition it ends in a prompt that asks you whether the test file should be overwritten:  &gt; Overwrite existing file `integration-tests/builtin-fromdate.md`?   Hit y here, which will cause scrut to update your test and add the missing output lines after the command for you.  Writing tests and using update to fill in the outputs is good practice for creating new tests and also for maintain existing ones: Imagine you fix a typo in the command output. Run scrut update &lt;file&gt; to fix the test. Does the typo change a lot of tests? Run scrut update &lt;directory&gt; and be done.  ","version":"Next","tagName":"h2"},{"title":"Powerful Expectations‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#powerful-expectations","content":" Take a step back and consider the test cases we wrote so far - and compare them against real-live scenarios. One thing may peak out you: Using a the commits.json file as a test fixture is a neat way to assure that we always work on the same input data. However, especially in the end-2-end testing space, things are not always possible. Things are not as neat and tidy.  Leave the idea of testing the functionality of jq for a moment behind, so you can think about writing tests for situations where the data your tests run on is outside of your control.  Let's revisit our transform-input.md test file from before. Copy it into transform-input-live.md and change in that new file the command into the following:  ```scrut $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ &gt; jq '.[] | .commit.author.name + &quot;;&quot; + .commit.committer.date' ```   This means: we are back to using the live data (to simulate &quot;dirty&quot; / unpredictable data). Also the output is no longer JSON, but a single line string per commit with the format &lt;name&gt;;&lt;date&gt;.  First, run scrut update on it and overwrite the contents. The modified transform-input-live.md file should look something like that (with different names and dates):  # Test transformation ## Bootstrap ```scrut $ source &quot;$TESTDIR/setup.sh&quot; ``` ## Transform input from live data ```scrut $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ &gt; jq '.[] | .commit.author.name + &quot;;&quot; + .commit.committer.date' Person Name;2022-05-26T21:04:32Z Another Person;2022-05-26T21:02:50Z Even More;2022-05-26T21:02:10Z And so forth;2022-05-26T21:01:25Z Name Name;2022-05-26T20:53:59Z ```   We already established, that having this specific content in there is brittle and will cause headache down the line. So where is this going?  At this point it becomes necessary to understand that each of the output lines in the test are actually output expectations. The last line of the above output could also be written as:  Name Name;2022-05-26T20:53:59Z (equal)   The tailing (equal) is the type, telling Scrut that this is, well, an expectation which should match exactly the provided expression (like the == equal operator). Since those are the most common ones, and it is so much more readable to not have (equal) everywhere, you can omit it. However, this the only expectation that allows you to omit the type.  ","version":"Next","tagName":"h2"},{"title":"Glob‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#glob","content":" Scrut has two expectation types that would work here. Lets start with simpler one, that is powerful, but not very precise, though easy to write and read. It is the glob expectation. Consider the following:  ## Transform input from live data ```scrut $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ &gt; jq '.[] | .commit.author.name + &quot;;&quot; + .commit.committer.date' *;20*Z (glob) *;20*Z (glob) *;20*Z (glob) *;20*Z (glob) *;20*Z (glob) ```   Without going into full detail, glob supports two wildcard characters * for any amount of any character and ? for a single arbitrary character. Each of the above expectations translates to:  Any string that is followed by ;20Followed by anythingEnding in Z  Note: anything means anything but a newline character  Using the glob expectation like this should cover about any possible output - at least until the year 2100. There should be little maintenance in the short- to midterm. That is reasonable resilient - but rather imprecise.  On that note: As you can see, we repeated the same expectation five times. Each line of output must have a matching expectation or the test fails. That also means: Having exactly five expectations is a test in itself, which would fail for zero or four or six lines of outputs equally.  ","version":"Next","tagName":"h3"},{"title":"Regular expression‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#regular-expression","content":" The above headline bestows fear in many and delight in some. So it is up to you to read this paragraph or skip it entirely. If you are not familiar with regular expressions, maybe you take this as an opportunity to learn about them - although this is way beyond the scope of this how-to.  Lets jump right into it then: scrut supports regular expression expectations with the (regex) type. Rewriting the test from above could look like that (well, one variant):  ## Transform input from live data ```scrut $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ &gt; jq '.[] | .commit.author.name + &quot;;&quot; + .commit.committer.date' \\w+(?:\\s\\w+)*;\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z (regex) \\w+(?:\\s\\w+)*;\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z (regex) \\w+(?:\\s\\w+)*;\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z (regex) \\w+(?:\\s\\w+)*;\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z (regex) \\w+(?:\\s+\\w+)*;\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z (regex) ```   This is much more precise than the above glob expectation - at the cost of readability. There is room for error, that likely won't capture all possible name writings (e.g. Forename M. Surname would fail) - feel free to optimize.  ","version":"Next","tagName":"h3"},{"title":"Quantifiers‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#quantifiers","content":" A last, but extremely useful feature - especially when testing multiple lines of similar formed output - are Quantifiers.  Consider the curl query from above. It ends in ?per_page=5, which indicates that we should expect up to five items - could be less, though. A different valid scenario would be too much output. Imagine your CLI outputs, say, hundreds or even thousands of lines. That would make any test file unreadable, aka unmaintainable, for humans. A test that cannot be understood is equal to no test - maybe even worse.  So how would a test look that addresses those issues? Especially when knowing that every output line must be covered by an expectation? Enter the expectation quantifier, which allows you to define quantities for expectations. Consider this:  ## Transform input from live data ```scrut $ curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | \\ &gt; jq '.[] | .commit.author.name + &quot;;&quot; + .commit.committer.date' *;20*Z (glob+) ```   Note the + symbol after the glob word. That is a quantifier. Read more about them here. Suffice to say that there are three (? = optional, * = 0 or more, + = 1 or more). Meaning, this single line covers all the possible output lines that match this form.  ","version":"Next","tagName":"h3"},{"title":"Pattern: Structure by use-case‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#pattern-structure-by-use-case","content":" This tutorial already talked about how to structure tests inside a file (having bootstrapping at the top, followed by the actual tests). As a last topic let's talk for a minute about how to structure test files (within folders).  As noted at the start of this document, Scrut can be very useful for CLI owners and system administrators alike. The former may concentrate on testing and documenting a single CLI. The latter may concentrate on testing and documenting the interplay of multiple command line tools at once, maybe the process of a runbook, or a specific operation to recover a database or something like that.  Either way it is good practice to isolate every use-case into a single file. That could be one test file per sub-command of the CLI that is tested or one test file per runbook that is tested. Whatever makes most sense. The purpose should be to gain the most information possible out of a failing test: Test A.md is failing, but test B.md is not, that indicates that feature X is broken.  For jq that could mean to write a single file per function jq exposes. However, if jq already has a unittest suite that covers each function, maybe it makes more sense to concentrate on testing I/O and also maybe whether modules work as expected.  ","version":"Next","tagName":"h2"},{"title":"Next steps‚Äã","type":1,"pageTitle":"Tutorial","url":"/scrut/docs/tutorial/#next-steps","content":" You did it. You are a scrutacean now (rust developers are called rustaceans, scrut is build in rust, there you go). If you want, go ahead and write some additional tests for jq, or dig deeper into the rest of file formats, expectations or behavioral specifics. ","version":"Next","tagName":"h2"}]