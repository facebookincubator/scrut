[{"title":"Getting Started","type":0,"sectionRef":"#","url":"/scrut/docs/getting-started/","content":"","keywords":"","version":"Next"},{"title":"Requirements‚Äã","type":1,"pageTitle":"Getting Started","url":"/scrut/docs/getting-started/#requirements","content":" Command Line Basics: Scrut is a command line tool that is made to test other command line tools. That means you should have at least some cursory shell knowledge to follow along.Markdown Basics: The tests will be written in Markdown documents, to you should have some basic understanding of Markdown syntax.Mac / Linux ( / Windows*): Besides that all you need is a Mac or Linux system and about ten minutes of your time.  note * Scrut fundamentally works on Windows, but the examples in the guides here focus on Mac / Linux terminals that run bash.  ","version":"Next","tagName":"h2"},{"title":"Start Here: Installation‚Äã","type":1,"pageTitle":"Getting Started","url":"/scrut/docs/getting-started/#start-here-installation","content":" Start with installing Scrut on your local development system. ","version":"Next","tagName":"h2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/scrut/docs/getting-started/installation/","content":"","keywords":"","version":"Next"},{"title":"Install via Script (Linux, Mac)‚Äã","type":1,"pageTitle":"Installation","url":"/scrut/docs/getting-started/installation/#install-via-script-linux-mac","content":" Execute the following from your shell:  Terminal $ curl --proto '=https' --tlsv1.2 -sSf https://facebookincubator.github.io/scrut/install.sh | sh   This will  Download and unpack the latest Scrut binaryInstall the binary in your local path (either ~/bin or ~/.local/bin, whichever exists)  The following parameters are supported:  Name\tDescription\tDefault--verbose, -v\tExplicitly log everything that is executed (set -x)\t- --owner-repo, -r\tGithub owner and repository in format OWNER/REPO\tfacebookincubator/scrut --installation-path, -p\tSet installation path\t$HOME/bin or $HOME/.local/bin  You can supply them by appending them like so:  Terminal $ curl --proto '=https' --tlsv1.2 -LsSf https://facebookincubator.github.io/scrut/install.sh | sh -s -- -p /my/install/directory   ","version":"Next","tagName":"h2"},{"title":"Install via Pre-Build Binaries (Linux, Mac, Windows)‚Äã","type":1,"pageTitle":"Installation","url":"/scrut/docs/getting-started/installation/#install-via-pre-build-binaries-linux-mac-windows","content":" Head over to https://github.com/facebookincubator/scrut/releases/latest and select the appropriate binary for your operating system.  Once downloaded and unpacked move the binary scrut (or scrut.exe on Windows) to a directory in your PATH.  ","version":"Next","tagName":"h2"},{"title":"Install via Cargo (Linux, Mac, Windows)‚Äã","type":1,"pageTitle":"Installation","url":"/scrut/docs/getting-started/installation/#install-via-cargo-linux-mac-windows","content":" You need to have a working Rust setup installed on your local machine. Then you can build and install the scrut binary as any other Rust binary:  $ cargo install scrut   This will install the scrut binary after building it in your local cargo binary folder (~/.cargo/bin on Linux and Mac, %USERPROFILE%\\.cargo\\bin on Windows).  If you want to install the binary manually then you need to check out the repository and then build it with:  $ cargo build --release --bin scrut   This will create target/release/scrut which you now can move to a directory in your PATH.  ","version":"Next","tagName":"h2"},{"title":"Install via Homebrew (Mac)‚Äã","type":1,"pageTitle":"Installation","url":"/scrut/docs/getting-started/installation/#install-via-homebrew-mac","content":" Coming soon  ","version":"Next","tagName":"h2"},{"title":"Verify‚Äã","type":1,"pageTitle":"Installation","url":"/scrut/docs/getting-started/installation/#verify","content":" Now that you have downloaded the binary and stored it in your PATH verify that you can execute the following before proceeding:  Terminal $ scrut --version scrut v0.X.Y   (You will see the latest version here) ","version":"Next","tagName":"h2"},{"title":"Dotslash and Version Pinning","type":0,"sectionRef":"#","url":"/scrut/docs/integration/dotslash/","content":"Dotslash and Version Pinning On the GitHub release page of Scrut you will find a file named scrut. This is a Dotslash file. Dotslash is a command line tool that is designed to fetch, verify and execute arbitrary other command line tools. Using the Dotslash allows you to pin a specific version of Scrut in your automation (i.e. CI/CD pipeline) without storing the Scrut binary itself. It also automatically checks the hash of the downloaded file to ensure that the file is not corrupted. Assuming dotslash is installed in your system, then you can: # decide on the version $ export SCRUT_VERSION=v0.3.0 # Download the latest (or specific version) of the Scrut Dotslash file $ curl -L https://github.com/facebookincubator/scrut/releases/download/${SCRUT_VERSION}/scrut &gt; scrut $ chmod +x scrut # Execute Scrut via Dotslash $ ./scrut test some/file.md tip DotSlash files provide download instructions for multiple operating systems. The Scrut Dotslash file is configured to work on Mac (ARM64 and x86_64), Linux (ARM64 and x86_64) and Windows (x86_64). That makes it very easy to run Scrut tests in multi-platform scenarios.","keywords":"","version":"Next"},{"title":"Scrut in GitHub Action","type":0,"sectionRef":"#","url":"/scrut/docs/integration/github-action/","content":"Scrut in GitHub Action Currently there is no official Scrut GitHub Action, but you can manually run Scrut. Following an example how that can look like: Runs on push and on PRsChecks out the current codeInstalls DotSlashDownloads a pinned version of ScrutRuns all test documents in the my-test-folder/ directory with Scrut .github/workflows/scrut-tests.yml name: Scrut Tests on: [push, pull_request] jobs: scrut-tests: runs-on: ubuntu-latest steps: - name: Checkout Code uses: actions/checkout@v3 - name: Setup DotSlash uses: facebook/install-dotslash@latest - name: Download Scrut DotSlash Wrapper env: SCRUT_VERSION: v0.3.0 run: | curl --proto '=https' --tlsv1.2 -LsSf \\ &quot;https://github.com/facebookincubator/scrut/releases/download/$SCRUT_VERSION/scrut&quot; \\ &gt; scrut chmod +x scrut - name: Run Scrut Tests run: ./scrut test my-test-folder/ note If you wish to use the latest version of Scrut, instead of a pinned one, use the URL https://github.com/facebookincubator/scrut/releases/latest/download/scrut instead.You could, of course, download and unpack the Scrut binary directly, but then you have to consider the OS (DotSlash does this for you) and it will be slightly more boilerplate code.","keywords":"","version":"Next"},{"title":"First Steps","type":0,"sectionRef":"#","url":"/scrut/docs/getting-started/first-steps/","content":"","keywords":"","version":"Next"},{"title":"Create your first test‚Äã","type":1,"pageTitle":"First Steps","url":"/scrut/docs/getting-started/first-steps/#create-your-first-test","content":" Create a new directory you want to work in. Then create a markdown file named getting-started.md in that directory with the following contents:  getting-started.md # Get Testing All code blocks marked with `scrut` are tests. ## Hello World ```scrut $ echo Hello World Hello World ``` This will work! ## Sad World ```scrut $ echo Sad World Hello World ``` This will fail! ## Ignore World ```other $ echo Other World Hello World ``` This is ignored.   Don't worry about the contents for now. All will be explained later. Open a terminal in the directory where you created the markdown file and run a test:  Terminal $ scrut test getting-started.md üîé Found 1 test document(s) ‚ùå getting-started.md: failed 1 out of 2 testcases // ============================================================================= // @ getting-started.md:17 // ----------------------------------------------------------------------------- // # Sad World // ----------------------------------------------------------------------------- // $ echo Sad World // ============================================================================= 1 | - Hello World 1 | + Sad World Result: 1 document(s) with 2 testcase(s): 1 succeeded, 1 failed and 0 skipped   It failed. Great! That is expected.  ","version":"Next","tagName":"h2"},{"title":"What just happened?‚Äã","type":1,"pageTitle":"First Steps","url":"/scrut/docs/getting-started/first-steps/#what-just-happened","content":" You ran your first test. Let's walk through it, starting with the command you executed:  $ scrut test getting-started.md   This line should be self-explanatory, but let's be explicit:  scrut is your previsouly installed Scrut binarytest is the subcommand that tells Scrut to run testsgetting-started.md is the path to the file you just created, telling Scrut to run the tests defined within  üîé Found 1 test document(s)   Scrut reports that it found one test document.  info Scrut works with files that contains tests. That makes the word test ambiguous: Does that refer to the file? Or to a test within the file? To make it clear what is mean Scrut uses document or test document to refer to the file that contains tests and test case to refer to a test within a test document.  ‚ùå getting-started.md: failed 1 out of 2 testcases   This line points to a test document (getting-started.md) and lets you know that one of the two test cases in that file have failed.  Next up is the output that starts with //, which you will see whenever there is a failure in a testcase:  // @ getting-started.md:17   An error occured in the command that can be found in line 17 of the getting-started.md file.  // # Sad World   The failing test title (i.e. the headline preceeding the code block, see the getting-started.md file above).  // $ echo Sad World   The shell expression that was executed and ended in failure.  info Scrut calls the commands that are being tested shell expressions. Each test contains a single shell expression. It may span across multiple lines. More about that later.  1 | - Hello World   The expected output, as defined in getting-started.md below the test command. The 1 signifies that this is the first expected output line, measured after the command that is tested (echo Sad World).   1 | + Sad World   The actual output, that did not match the expection. The 1 signfies that this is the first output line, measured from the output of the command that is tested (echo Sad World).  Result: 1 document(s) with 2 testcase(s): 2 succeeded, 0 failed and 0 skipped   The test file contained 2 tests, one succeeded and one failed. Note that only the code blocks marked with scrut were considered. The last code block in the getting-started.md file that has the &quot;langauge&quot; other set was (intentionally) ignored.  In summary:  You told Scrut to run tests in a file (getting-started.md)One of the tests in that file failedScrut told you which test failed, where in the file it is and how it did not match the expectations that are defined in the file  ","version":"Next","tagName":"h2"},{"title":"Fix it!‚Äã","type":1,"pageTitle":"First Steps","url":"/scrut/docs/getting-started/first-steps/#fix-it","content":" To fix the test you need to understand a bit more about the syntax in the getting-started.md file. Have a look at the second code block in the file that reads:  ```scrut $ echo Sad World Hello World ```   The first line here that starts with a $ (dollar) sign is, as mentioned before, the shell expression of the testcase. The next line is what validates the output of the execution. Scrut calls this a output expectation. More on that later.  You can read the test as following:When I execute echo Sad World, then I expect to see Hello World printed.  Obviously that expectation is wrong: The expected output for echo Sad World is Sad World, not Hello World.  So in order to fix this simple test, all you need to do is align either (not both!) of the following:  Use $ echo Hello World as the shell expression.Use Sad World as the test expectation.  That means either of the following tests are valid:  ```scrut $ echo Hello World Hello World ```   or  ```scrut $ echo Sad World Sad World ```   Once you have &quot;fixed&quot; the test subsequent scrut test execcutions will succeed:  Terminal $ scrut test getting-started.md üîé Found 1 test document(s) Result: 1 document(s) with 2 testcase(s): 2 succeeded, 0 failed and 0 skipped   info Scrut by default only prints tests that did not succeed. The Result: line is always printed and shows you how many documents processed, but if you want to see the tests as they are being processed use the --verbose flag: Terminal $ scrut test --verbose getting-started.md üîé Found 1 test document(s) ‚úÖ /tmp/getting-started.md: passed 2 testcases Result: 1 document(s) with 2 testcase(s): 2 succeeded, 0 failed and 0 skipped   ","version":"Next","tagName":"h2"},{"title":"Next Up‚Äã","type":1,"pageTitle":"First Steps","url":"/scrut/docs/getting-started/first-steps/#next-up","content":" Congratulations! You have taken the first step, which is always the hardest.  Go ahead and start with the tutorial. ","version":"Next","tagName":"h2"},{"title":"What is Scrut?","type":0,"sectionRef":"#","url":"/scrut/docs/","content":"","keywords":"","version":"Next"},{"title":"Contribute‚Äã","type":1,"pageTitle":"What is Scrut?","url":"/scrut/docs/#contribute","content":"CONTRIBUTING.mdCODE_OF_CONDUCT.md ","version":"Next","tagName":"h2"},{"title":"License‚Äã","type":1,"pageTitle":"What is Scrut?","url":"/scrut/docs/#license","content":"LICENSE ","version":"Next","tagName":"h2"},{"title":"Scrut in Docker Container","type":0,"sectionRef":"#","url":"/scrut/docs/integration/docker/","content":"","keywords":"","version":"Next"},{"title":"Get Scrut Docker Image‚Äã","type":1,"pageTitle":"Scrut in Docker Container","url":"/scrut/docs/integration/docker/#get-scrut-docker-image","content":" There are two ways:  ","version":"Next","tagName":"h2"},{"title":"Pre-Built Image from GHCR‚Äã","type":1,"pageTitle":"Scrut in Docker Container","url":"/scrut/docs/integration/docker/#pre-built-image-from-ghcr","content":" Here is how you can work with theGitHub Container Registry.  The image is then available as:  ghcr.io/facebookexternal/scrut:&lt;VERSION&gt;   ","version":"Next","tagName":"h3"},{"title":"Build Locally‚Äã","type":1,"pageTitle":"Scrut in Docker Container","url":"/scrut/docs/integration/docker/#build-locally","content":" Check out the Scrut git repository on GitHub locally. It comes with a Dockerfile in the root directory.  Now build the image:  $ docker build -t scrut:latest .   note The build requires Docker BuildKit.  tip The container build automatically runs both unit and integrating tests. This makes it a good, isolated development environment if you are interested in contributing to Scrut. If you want to skip the tests, resulting in a faster build, you can set --build-arg SKIP_TESTS=yes when executing docker build.  ","version":"Next","tagName":"h3"},{"title":"Run Scrut in Docker Container‚Äã","type":1,"pageTitle":"Scrut in Docker Container","url":"/scrut/docs/integration/docker/#run-scrut-in-docker-container","content":" Once you have the image available make sure to mount the directory containing the test suite as a volume into the container under /app.  Following an example with a small Rust CLI:  $ cd my-cli $ tree . ‚îú‚îÄ‚îÄ Cargo.toml ‚îú‚îÄ‚îÄ dist ‚îÇ ‚îî‚îÄ‚îÄ my-cli ‚îú‚îÄ‚îÄ src ‚îÇ ‚îú‚îÄ‚îÄ command_something_else.rs ‚îÇ ‚îú‚îÄ‚îÄ command_user_list.rs ‚îÇ ‚îú‚îÄ‚îÄ command_user_login.rs ‚îÇ ‚îî‚îÄ‚îÄ main.rs ‚îî‚îÄ‚îÄ tests ‚îú‚îÄ‚îÄ smoke.md ‚îú‚îÄ‚îÄ something-else.md ‚îú‚îÄ‚îÄ user-listing.md ‚îî‚îÄ‚îÄ user-login.md   Now you would run Scrut like this:  Terminal $ docker run --rm -ti -v $(pwd):/app scrut:latest test --verbose tests/ üîé Found 4 test document(s) ‚úÖ tests/user-login.md: passed 3 testcases ‚úÖ tests/smoke.md: passed 5 testcases ‚úÖ tests/user-listing.md: passed 1 testcase ‚úÖ tests/something-else.md: passed 13 testcases Result: 4 document(s) with 22 testcase(s): 22 succeeded, 0 failed and 0 skipped   tip Running tests inside a container can change the path location of the CLI binary. Consider using the --prepend-test-file-paths parameter to inject a test document that extends the PATH environment variable as needed. Here an example: docker-prepend.md # Add `/app/dist` to `PATH` ```scrut $ export PATH=&quot;/app/dist:$PATH&quot; ``` And then all calls to my-cli in the test documents will be resolved to /app/dist/my-cli: Terminal $ docker run --rm -ti -v $(pwd):/app scrut:latest \\ test --verbose --prepend-test-file-paths=./docker-prepend.md tests/ üîé Found 4 test document(s) ‚úÖ tests/user-login.md: passed 4 testcases ‚úÖ tests/smoke.md: passed 6 testcases ‚úÖ tests/user-listing.md: passed 2 testcase ‚úÖ tests/something-else.md: passed 14 testcases Result: 4 document(s) with 26 testcase(s): 26 succeeded, 0 failed and 0 skipped  ","version":"Next","tagName":"h2"},{"title":"Execution Model","type":0,"sectionRef":"#","url":"/scrut/docs/reference/behavior/execution-model/","content":"","keywords":"","version":"Next"},{"title":"Shared Shell Environment‚Äã","type":1,"pageTitle":"Execution Model","url":"/scrut/docs/reference/behavior/execution-model/#shared-shell-environment","content":" Each subsequent test case in the same document inherits the shell environment of the previous test case. This means: All environment variables, shell variables, aliases, functions, etc that have been set in one test case are available to the immediate following test case.  E.g. export FOO=bar in one test case will still be set in the following test case.Exception: Environments set in detached test cases are not inherited.  ","version":"Next","tagName":"h2"},{"title":"Shared Ephemeral Directories‚Äã","type":1,"pageTitle":"Execution Model","url":"/scrut/docs/reference/behavior/execution-model/#shared-ephemeral-directories","content":" Each test cases in the same document executes in the the same working directory and is provided with the same temporary directory ($TEMPDIR). Both directories will be removed (cleaned up) after test execution - independent of whether the test execution succeeds or fails.  Exception: If the --work-directory command-line parameter is provided, then this directory will not be cleaned up (deleted) after execution. A temporary directory, that still will be removed after execution, will be created within the working directory.  ","version":"Next","tagName":"h2"},{"title":"Process Isolation‚Äã","type":1,"pageTitle":"Execution Model","url":"/scrut/docs/reference/behavior/execution-model/#process-isolation","content":" Scrut starts individual bash processes for executing each shell expression of each test case in the same document. The environment of the previous execution is pulled in through a shared state file, that contains all environment variables, shell variables, aliases, functions and settings as they were set when the the previous test case execution ended.  Markdown vs Cram Markdown is the default Scrut test document format. Cram is supported for legacy reasons. Hence it's legacy mode of execution is also respected. The main difference in Cram from the above is: Each execution from the same test document is executed in the same shell process. This is less flexible (e.g. Scrut cannot constraint max execution time per test case) and more prone to unintended side-effects (e.g. set -e terminating all test executions, not only a single test case or detached processes interfering with output association to specific tests). We recommend to use Markdown. ","version":"Next","tagName":"h2"},{"title":"STDOUT and STDERR","type":0,"sectionRef":"#","url":"/scrut/docs/reference/behavior/stdout-and-stderr/","content":"STDOUT and STDERR Commands-line applications can generate output on to two streams: STDOUT and STDERR. There is no general agreement on which stream is supposed to contain what kind of data, but commonly STDOUT contains the primary output and STDERR contains logs, debug messages, etc. This is also the recommendation of the CLI guidelines. Scrut, by default, only considers STDOUT when validating output. You can modify this behavior by using the output_stream configuration directive or the --(no-)combine-output command-line parameters. tip While you can configure which output streams Scrut considers when evaluating output expecations, you can also steer this by using stream control bash primitives like some-command 2&gt;&amp;1. note The above is true for Markdown test documents. However Cram test documents default to combining STDOUT and STDERR.","keywords":"","version":"Next"},{"title":"Newline handling","type":0,"sectionRef":"#","url":"/scrut/docs/reference/behavior/newline-handling/","content":"Newline handling Newline endings is a sad story in computer history. In Unix / MacOS ( / *BSD / Amiga / ...) the standard line ending is the line feed (LF) character \\n. Microsoft DOS (also Palm OS and OS/2?) infamously attempted to make a combination of carriage return (CR) and line feed the standard: CRLF (\\r\\n). This made everybody mad - and they still are. See the keep_crlf configuration directive to understand how Scrut handles LF and CRLF and how you can modify the default behavior.","keywords":"","version":"Next"},{"title":"Exit Codes","type":0,"sectionRef":"#","url":"/scrut/docs/reference/behavior/exit-codes/","content":"","keywords":"","version":"Next"},{"title":"Skip Tests with Exit Code 80‚Äã","type":1,"pageTitle":"Exit Codes","url":"/scrut/docs/reference/behavior/exit-codes/#skip-tests-with-exit-code-80","content":" If any test case in a test file exist with exit code 80, then all test case in that file are skipped.  This is especially helpful for OS specific tests etc. Imagine:  example.md Run tests in this file only on Mac ```scrut $ [[ &quot;$(uname)&quot; == &quot;Darwin&quot; ]] || exit 80 ```   note The exit code can be configured with the skip_document_code configuration directive.  ","version":"Next","tagName":"h2"},{"title":"Scrut Exit Code‚Äã","type":1,"pageTitle":"Exit Codes","url":"/scrut/docs/reference/behavior/exit-codes/#scrut-exit-code","content":" Scrut itself communicates the outcome of executions with exit codes. Currently three possible exit codes are used:  0: Command succeeded, all is good (scrut test, scrut create, scrut update)1: Command failed with error (scrut test, scrut create, scrut update)50: Validation failed (scrut test only) ","version":"Next","tagName":"h2"},{"title":"Custom Shell","type":0,"sectionRef":"#","url":"/scrut/docs/reference/custom-shell/","content":"Custom Shell While Scrut currently only supports bash (&gt;= 3.2) a custom shell can be provided with the --shell command line parameter. To understand how that works consider the following: Terminal $ echo &quot;echo Hello&quot; | /bin/bash - Hello What the above does is piping the string echo Hello into the STDIN of the process that was started with /bin/bash -. Scrut pretty much does the same with each shell expressions within a test file. So why provide a custom --shell then? This becomes useful in at least two scenarios: You need to execute the same code before Scrut runs each individual expressionYou need Scrut to redirect the execution, for example an isolated environment For (1) consider the following code: my_custom_setup.sh #!/bin/bash # do something in this wrapper script source /my/custom/setup.sh run_my_custom_setup # consume and run STDIN source /dev/stdin For (2) consider the following: my_remote_execution.sh #!/bin/bash # do something in this wrapper script source /my/custom/setup.sh run_my_custom_setup # end in a bash process that will receive STDIN exec ssh username@acme.tld /bin/bash ","keywords":"","version":"Next"},{"title":"Working Directory","type":0,"sectionRef":"#","url":"/scrut/docs/reference/behavior/working-directory/","content":"Working Directory By default Scrut executes all tests in a dedicated directory per test document. This means all test cases within one document are being executed in the same directory. The directory is created within the system temporary directory. It will be removed (including all the files or directories that the tests may have created) after all tests in the file are executed - or if the execution of the file fails for any reason. This means something like the following can be safely done and will be cleaned up by Scrut after the test finished (however it finishes): test.md # Some test that creates a file ```scrut $ date &gt; file ``` The `file` lives in the current directory ```scrut $ test -f &quot;$(pwd)/file&quot; ``` The directory within which tests are being executed can be explicitly set using the --work-directory parameter for the test and update commands. If that parameter is set then all tests from all test files are executed run within that directory, and the directory is not removed afterwards. note Consider also the environment variables TESTDIR and TMPDIR described in Reference &gt; Fundamentals &gt; Environment Variables.","keywords":"","version":"Next"},{"title":"Environment Variables","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/environment-variables/","content":"","keywords":"","version":"Next"},{"title":"Scrut specific environment variables‚Äã","type":1,"pageTitle":"Environment Variables","url":"/scrut/docs/reference/fundamentals/environment-variables/#scrut-specific-environment-variables","content":" TESTDIR: absolute path of the directory where the document that contains the test that is currently being executed is inTESTFILE: name of the test document that contains the test that is currently being executedTESTSHELL: shell that in which the test is being executed in (default /bin/bash, see --shell flag on commands)TMPDIR: absolute path to a temporary directory that will be cleaned up after the test is executed. This directory is shared in between all executed tests across all test documents. Tools like mktemp will make use of TMPDIR automatically.SCRUT_TEST: path to the test document and the line number, separated by a colon (e.g. some/test.md:123). This variable is recommend to use when deciding whether an execution is within Scrut.  tip Use the SCRUT_TEST variable to decide whether an execution is within Scrut. This is useful when you want to be aware of that fact from within your CLI during test execution.  ","version":"Next","tagName":"h2"},{"title":"Common (linux) environment variables‚Äã","type":1,"pageTitle":"Environment Variables","url":"/scrut/docs/reference/fundamentals/environment-variables/#common-linux-environment-variables","content":" Scrut sets the following environment variables to their default values:  CDPATH: emptyCOLUMNS: 80GREP_OPTIONS: emptyLANG: CLANGUAGE: CLC_ALL: CSHELL: Same as TESTSHELL, see aboveTZ: GMT  ","version":"Next","tagName":"h2"},{"title":"(Optional) Cram environment variables‚Äã","type":1,"pageTitle":"Environment Variables","url":"/scrut/docs/reference/fundamentals/environment-variables/#optional-cram-environment-variables","content":" When using the --cram-compat flag, or when a Cram .t test document is being executed, the following additional environment variables will be exposed for compatibility:  CRAMTMP: if no specific working directory was provided (default), then it contains the absolute path to the temporary directory in which per-test-file directories will be created in which those test files are then executed in (CRAMTMP=$(realpath &quot;$(pwd)/..&quot;)); otherwise the path to the provided working directoryTMP: same as TMPDIRTEMP: same as TMPDIR  ","version":"Next","tagName":"h2"},{"title":"Controlling Environment Variables‚Äã","type":1,"pageTitle":"Environment Variables","url":"/scrut/docs/reference/fundamentals/environment-variables/#controlling-environment-variables","content":" All environment variables above are exported by Scrut for test execution. Additionally there are environment variables that can be used to control Scrut's behavior:  SCRUT_DEFAULT_SHELL: Overrides the path to the default shell that is used for executing shell expressions. This is useful when the bash binary is located in a non-standard location. ","version":"Next","tagName":"h2"},{"title":"Cram Format","type":0,"sectionRef":"#","url":"/scrut/docs/reference/formats/cram-format/","content":"Cram Format warning For new tests, prefer using the Markdown format which was introduced with two goals in mind: Tests ‚ù§Ô∏è Documentation: The value of tests is not only in proving behavior, but also in documenting it - and thereby also in teaching it. The Markdown format allows you to keep tests around in a way that future generations of maintainers will love you for.Bad Spaces üëæ: To denote an expected empty line of output in Cram format you have to provide two empty spaces . This goes counter a lot of default behavior in the development toolchain. Many CI/CD tools are tuned to automatically ignore changes that only affect white spaces. Code review tools often deliberately hide white spae changes. White spaces are generally hard to see in code editors - if they are visualized at all. Breaking tests that are caused by an accidentally removed or added space cause rage quitting. See also additional differences in the way tests are executed. The Cram document format is supported for legacy reasons. The general guidance to write test cases in Cram test documents is: The first line of a shell expression must start with $ (space + space + dollar + space), any subsequent with &gt; (space + space + closing angle bracket + space). This is different from Markdown Scrut syntax. Be mindful of the additional spaces. Lines following the shell expression, that are also indented with two spaces, are considered output expectations If an exit code other than 0 is expected, it can be denoted in square brackets [123] once per test caseNote: Empty output lines (=empty shell expectations) must still have two leading space charactersNote: A fully empty line (no leading spaces) denotes the end of the current test case If the shell expression is preceded by a non-empty line (that is not indented) the line is considered the title of the test case Here an example: This is a comment $ scrut --help Scrut help output Another test case in the same document $ scrut --version Scrut version output Multiple tests test cases can be written in sequence without any empty lines in between: A title for the first test case $ first --command $ second --command $ third --command Output Expectation ","keywords":"","version":"Next"},{"title":"Markdown Format","type":0,"sectionRef":"#","url":"/scrut/docs/reference/formats/markdown-format/","content":"","keywords":"","version":"Next"},{"title":"Test Case Anatomy‚Äã","type":1,"pageTitle":"Markdown Format","url":"/scrut/docs/reference/formats/markdown-format/#test-case-anatomy","content":" A test case in Markdown is structured as follows:  shell expressions and output expectations live in the same code-block, that must be annotated with the language scrut The first line of a shell expressions must start with $ (dollar, sign followed by a space), any subsequent with &gt; (closing angle bracket / chevron, followed by a space)All other lines in the code block (including empty ones) that follow the shell expression are considered output expectationsLines starting with # that precede the shell expression are ignored (comments)If an exit code other than 0 is expected, it can be denoted in square brackets [123] once per test case The first line before the code block that is either a paragraph or a header will be used as the title of the test case  Here an example:  This is the title ```scrut $ command | \\ &gt; other-command expected output line another expected output line [123] ```   ","version":"Next","tagName":"h2"},{"title":"Constraints‚Äã","type":1,"pageTitle":"Markdown Format","url":"/scrut/docs/reference/formats/markdown-format/#constraints","content":" The following constraints apply:  A markdown document can contain as many test cases as needed (0..n)Each code block in a test case may only have one (1) shell expression (each test case is considered atomic)Code blocks that do not denote a language (or a different language than scrut) will be ignored  With that in mind, consider the following markdown document that contains not only test cases but arbitrary other text and other code blocks. This is idiomatic Scrut markdown document that combines tests and documentation:  # This is just regular markdown It contains both Scrut tests **and** abitrary text, including code examples, that are unrelated to Scrut. ```python import os print(&quot;This code block ignored by Scrut&quot;) ``` ## Here is a scrut test ```scrut $ echo Hello Hello ``` ## Embedded with other documentation So it's a mix of test and not tests. Any amount of tests are fine: ```scrut $ echo World World ``` Just make sure to write only one [test case](/docs/reference/fundamentals/test-case/) per code-block.   note If you are testing actual markdown output, be aware that you can embed code blocks in other code blocks, if the outer code block uses one more backtick (opening and closing!) than the embedded one(s). Just have a look at the source code of this document right above this text.  ","version":"Next","tagName":"h2"},{"title":"Configuration‚Äã","type":1,"pageTitle":"Markdown Format","url":"/scrut/docs/reference/formats/markdown-format/#configuration","content":" Markdown test documents may contain inline configuration. Read more in Reference &gt; Fundamentals &gt; Inline Configuration. ","version":"Next","tagName":"h2"},{"title":"Test Case","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/test-case/","content":"","keywords":"","version":"Next"},{"title":"Anatomy‚Äã","type":1,"pageTitle":"Test Case","url":"/scrut/docs/reference/fundamentals/test-case/#anatomy","content":" Independent of the format (Markdown, Cram) that a test case is written, it consists of the following components:  Component\tRequired\tDescriptionTitle\tNo\tAn optional title for the test case, so that a human can understand what the test case is intended to prove. Comment\tNo\tAn optional comment leaving space for more description Shell Expression\tYes\tThe subject of the test (&quot;that what is being tested&quot;). Output Expectations\tNo\tAny amount of assertions of the output that the shell expression will print Exit Code\tNo\tThe expected exit code that the shell expression must end in. Configuration\tNo\tDetailed, per-test-case configuration.  ","version":"Next","tagName":"h2"},{"title":"Format‚Äã","type":1,"pageTitle":"Test Case","url":"/scrut/docs/reference/fundamentals/test-case/#format","content":" Find more about the formatting of test cases in:  Reference &gt; Formats &gt; Markdown FormatReference &gt; Formats &gt; Cram Format ","version":"Next","tagName":"h2"},{"title":"Test Document","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/test-document/","content":"","keywords":"","version":"Next"},{"title":"Document Formats‚Äã","type":1,"pageTitle":"Test Document","url":"/scrut/docs/reference/fundamentals/test-document/#document-formats","content":" Scrut supports two formats for test documents:  Markdown, the default and recommended format for writing test documents.Cram, supported for legacy reasons to run or migrate tests written for the now deprecated Cram framework  ","version":"Next","tagName":"h2"},{"title":"Document Writing Recommendations‚Äã","type":1,"pageTitle":"Test Document","url":"/scrut/docs/reference/fundamentals/test-document/#document-writing-recommendations","content":" Consider test documents not only as stashes for test cases, but also as documentation for the tested functionality. Maintaining CLIs, as any software, long term is a challenge. Using the Markdown format Scrut provides an opportunity to store knowledge about systems (i.e. behavior of the CLI) together with validation of that knowledge (the test cases).  ","version":"Next","tagName":"h2"},{"title":"File Structure Recommendation‚Äã","type":1,"pageTitle":"Test Document","url":"/scrut/docs/reference/fundamentals/test-document/#file-structure-recommendation","content":" There are two common patterns to structure test documents in Scrut:  Coherent Test Suite (recommended): One test file represents one use-case or behavior. This makes it easy to identify broken functionality.List of Tests: One test file contains a list of simple, not necessarily related tests. This makes it easy to cover a lot of functionality quickly, but at the price of harder maintainability down the line. ","version":"Next","tagName":"h2"},{"title":"Shell Expressions","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/shell-expression/","content":"","keywords":"","version":"Next"},{"title":"Constraints‚Äã","type":1,"pageTitle":"Shell Expressions","url":"/scrut/docs/reference/fundamentals/shell-expression/#constraints","content":" For the sake of understanding assume that each shell expression is written to a file and this file is then executed with bash. Like so:  Terminal $ echo 'echo My shell expression' &gt; shell-expression.sh $ bash shell-expression.sh My shell expression   note Learn about the how execution works in Reference &gt; Behavior &gt; Execution Model.  This behavior implies some limits / constraints on what you can expect from the result:  ","version":"Next","tagName":"h2"},{"title":"Returned Exit Code‚Äã","type":1,"pageTitle":"Shell Expressions","url":"/scrut/docs/reference/fundamentals/shell-expression/#returned-exit-code","content":" Consider the following shell expression:  $ false ; true   This executes the command false and then executes the command true. They are both separated by a ;, which makes them individual commands from the bash perspective. If you would have a bash script file with these contents and would execute it, then you would see the exit code 0:  Terminal $ echo 'false ; true' &gt; shell-expression.sh $ bash shell-expression.sh $ echo $? 0   That means the exit code of false (which is 1) is not surfaced, because the shell script itself continues to the next command (true). The returned exit code is simply the exit code of the last command in the shell script.  If this behavior is not desired (it may be), then you could use the &amp;&amp; operator instead of ;:  Terminal $ echo 'false &amp;&amp; true' &gt; shell-expression.sh $ bash shell-expression.sh $ echo $? 1   Alternatively, as these are bash scripts, you can also use the set -e directive to make the shell script exit on the first non-zero exit code:  Terminal $ echo 'set -e ; false ; true' &gt; shell-expression.sh $ bash shell-expression.sh $ echo $? 1   warning Due to the different execution model of Cram using set -e will terminate not only the test case, but all test cases in the same test document. Do not use it.  ","version":"Next","tagName":"h3"},{"title":"Detached Processes‚Äã","type":1,"pageTitle":"Shell Expressions","url":"/scrut/docs/reference/fundamentals/shell-expression/#detached-processes","content":" When Scrut runs a shell expression it will wait for the execution to finish, so that it can gather the exit code and the output and validate it as defined by the test case.  However, if the shell expression detaches from the shell, or spawns processes that are detached (or both) then Scrut will not wait for them. Scrut will not manage their lifetime at all.  tip If you need to test a server/client scenario, where first a server must be started and before the CLI test cases can execute then have a look at the detached/wait configuration directives. Here the detached_kill_signal can be specified to send a user-definedable signal to the detached process to terminate it. Note that Scrut will only send the signal, it is up to the process to handle it correctly. ","version":"Next","tagName":"h3"},{"title":"Test Output","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/test-output/","content":"","keywords":"","version":"Next"},{"title":"Pretty Renderer (default)‚Äã","type":1,"pageTitle":"Test Output","url":"/scrut/docs/reference/fundamentals/test-output/#pretty-renderer-default","content":" Scrut will always tell you what it did:  Terminal $ scrut test selftest/cases/regex.md üîé Found 1 test document(s) Result: 1 document(s) with 10 testcase(s): 10 succeeded, 0 failed and 0 skipped   In case of failure the pretty default renderer will provide a human-readable output that points you to the problem with the output:  Terminal $ scrut test a-failing-test.md üîé Found 1 test document(s) ‚ùå /tmp/test.md: failed 1 out of 1 testcase // ============================================================================= // @ a-failing-test.md:10 // ----------------------------------------------------------------------------- // # One conjunct expression // ----------------------------------------------------------------------------- // $ echo Foo &amp;&amp; \\ // echo Bar // ============================================================================= 1 1 | Foo 2 | - BAR 2 | + Bar 3 | + Baz Result: 1 document(s) with 1 testcase(s): 0 succeeded, 1 failed and 0 skipped   The failure output consists of two components:  The failure header, which consists of all initial lines that start with //, indicates the positionThe failure body, which consists of all the following lines, indicates the problem  Header  The header contains three relevant information. Given the above output:  @ a-failing-test.md:10, tells you that the test that failed is in the provided document a-failing-test.md and that the shell expression (that failed the test) starts in line ten of that file.# &lt;test title&gt;, gives you the optional title of the test in the document. If the test does not have a title, this line is omitted.$ &lt;test command&gt;, is the shell expression from the test document that is tested and that has failed.  note See Reference &gt; Fundamentals &gt; Test Case) to learn more about test case anatomy.  Body  There are two possible variants that the diff renderer may return:  Failed output expectationsFailed exit code expectation  The above output is a failed output expectations and you can read it as following:  1 1 | Foo: This line was printed as expected. The left hand 1 is the number of the output line and the right hand 1 is the number of the expectation. 2 | - BAR: This line was expected, but not printed. The left hand omitted number indicates that it was not found in output. The right hand number tells that this is the second expectation. The - before the line Bar emphasizes that this is a missed expectation.2 | + Bar: This line was printed and expected. The left hand 2 is the number of the output line and the right hand 3 is the number of the expectation.3 | + Baz: This line was printed unexpectedly. The left hand 3 is the number of the output line the omitted right hand number implies there is no expectation that covers it. The + before the line Zoing emphasizes that this is a &quot;surplus&quot; line.  note If you work with test files that contain a large amount of tests, then you may want to use the --absolute-line-numbers flag on the command line: instead of printing the relative line number for each test, as described above, it prints absolute line numbers from within the test file. Assuming the Foo expectation from above is in line 10 of a file, it would read 13 13 | Foo - and all subsequent output liens with respective aligned line numbers.  An example for the body of an exit code expectation:  unexpected exit code expected: 2 actual: 0 ## STDOUT #&gt; Foo ## STDERR   This should be mostly self-explanatory. Scrut does not provide any output expectation failures, because it assumes that when the exit code is different, then it is highly likely that the output is very different - and even if not, it would not matter, as it failed anyway.  The tailing ## STDOUT and ## STDERR contain the output lines (prefixed with #&gt; ) that were printed out from the failed execution.  ","version":"Next","tagName":"h2"},{"title":"Diff renderer‚Äã","type":1,"pageTitle":"Test Output","url":"/scrut/docs/reference/fundamentals/test-output/#diff-renderer","content":" The diff renderer, that can be enabled with --renderer diff (or -r diff), prints a diff in the unified format.  Terminal $ scrut test -r diff a-failing-test.md üîé Found 1 test document(s) ‚ùå a-failing-test.md: failed 1 out of 1 testcase --- a-failing-test.md +++ a-failing-test.md.new @@ -14 +14,2 @@ malformed output: One conjunct expression -BAR +Bar +Baz   tip The created diff is compatible with the patch command line tool (e.g. patch -p0 &lt; &lt;(scrut test -r diff a-failing-test.md)). This is mostly equivalent to using the scrut update command.  ","version":"Next","tagName":"h2"},{"title":"JSON and YAML renderer‚Äã","type":1,"pageTitle":"Test Output","url":"/scrut/docs/reference/fundamentals/test-output/#json-and-yaml-renderer","content":" These renderer are primarily intended for automation and are to be considered experimental. You can explore them using --renderer yaml or respective --renderer json. ","version":"Next","tagName":"h2"},{"title":"In-Depth Tutorial","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/","content":"","keywords":"","version":"Next"},{"title":"About File Structure‚Äã","type":1,"pageTitle":"In-Depth Tutorial","url":"/scrut/docs/tutorial/#about-file-structure","content":" Scrut does not require any particular file structure. This tutorial is assuming that the files would be stored in a tests subdirectory together with the source code of the CLI that is being tested.  # Go to the directory where your CLI code lives $ cd ~/Projects/MyCLI # create a test folder $ mkdir tests   tip Although Scrut has no requirements towards file structure it is recommended that test relating files (like test fixtures, more about that later) are in the same directory structure as the test files themselves. This will make referencing them easier.  ","version":"Next","tagName":"h2"},{"title":"Using jq as an Example‚Äã","type":1,"pageTitle":"In-Depth Tutorial","url":"/scrut/docs/tutorial/#using-jq-as-an-example","content":" Since Scrut is a CLI testing framework we need a CLI to test. Hence this tutorial will use the powerful jq command-line JSON processor CLI as an example CLI to write tests for. Make sure you have it installed if you want to follow along. ","version":"Next","tagName":"h2"},{"title":"Output Expectations","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/output-expectations/","content":"","keywords":"","version":"Next"},{"title":"Quantifiers‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#quantifiers","content":" The Quantifiers can be understood as following (nothing new if you are familiar with regular expressions):  ?: Zero or one occurrence; basically an optional output line*: Any amount of occurrences (0..n); no line, one line, more lines - all good+: One or more occurrences (1..n); at least one line, more are fine  Quantifiers can be used with most expectations, see the examples and description below for more details.  ","version":"Next","tagName":"h2"},{"title":"Equal Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#equal-expectation","content":" The Equal Expectation denotes a single line of output that ends in a newline character. Because this expectation is the most common one you do not need to provide the specific kind. Here an example:  # Some Test ```scrut $ echo Hello Hello ```   The line that consists only of Hello is the Equal Expectation and specifies that the (first line of the) output must be equal to Hello\\n (with \\n being the newline of the operating system).  An extended for of the same Equal Expectation with explicit kind works as well and looks like that:  # Some Test ```scrut $ echo Hello Hello (equal) ```   The explicit form makes most sense in conjunction with quantifiers:  # Some Test ```scrut $ echo -e &quot;Hello\\nHello\\nHello&quot; Hello (equal+) ```   ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#examples","content":" Expression\tMeaningHello\tOne output line of the form Hello\\n Hello (equal)\tOne output line of the form Hello\\n Hello (?)\tOptional (zero or one) output line of the form Hello\\n Hello (*)\tAny amount (0..n) of output lines of the form Hello\\n Hello (+)\tOne or more (1..n) of output lines of the form Hello\\n Hello (equal*)\tAny amount (0..n) of output lines of the form Hello\\n Hello (equal+)\tOne or more (1..n) of output lines of the form Hello\\n  note You can use eq as a shorthand for equal  ","version":"Next","tagName":"h3"},{"title":"Equal No EOL Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#equal-no-eol-expectation","content":" Very close to the above, but much rarer, the Equal No EOL Expectation matches lines that do not end in a newline. Consider:  # Some Test ```scrut $ echo -n Hello Hello (no-eol) ```   The above echo -n Hello prints Hello without a tailing newline character (there is no \\n at the end of Hello).  This Expectation could possibly only be the last line of output, so quantifiers make little sense.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#examples-1","content":" Expression\tMeaningHello (no-eol)\tOne output line of the form Hello - a line that does not end in newline  ","version":"Next","tagName":"h3"},{"title":"Glob Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#glob-expectation","content":" Glob Expectations are support two wildcard characters:  ? matches exactly one occurrence of any character* matches arbitrary many (including zero) occurrences of any character  Together with quantifiers, this allows for powerful if imprecise matches of output lines.  # This will work ```scrut $ echo Hello You Hello* (glob) ``` This will work, too ```scrut $ echo -e &quot;Hello\\nHello There\\nHello World&quot; Hello* (glob+) ```   ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#examples-2","content":" Expression\tMeaningHello? (glob)\tA single output line that starts with Hello followed by one character Hello* (glob)\tA single output line that starts with Hello *Hello* (glob)\tA single output line that contains Hello *Hello (glob)\tA single output line that ends with Hello *Hello* (glob?)\tAn optional output line that contains Hello *Hello* (glob*)\tAny amount (0..n) of output lines that contain Hello *Hello* (glob+)\tOne or more (1..n) of output lines that contain Hello  note You can use gl as a shorthand for glob.Escaping, like Hello\\* * (glob), is not supported by the used library.  ","version":"Next","tagName":"h3"},{"title":"Regex Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#regex-expectation","content":" Regular Expressions are the most powerful and precise output describing rules that are supported. That comes at the price of complexity. Explaining regular expression syntax literally fills books, so here is not the place to attempt that.  Nonetheless, an obligatory example:  # This will work ```scrut $ echo Hello You Hello.+ (regex) ``` This will work, too: ```scrut $ echo -e &quot;Hello\\nEnding in Hello\\nHello Start&quot; .*Hello.* (regex+) ```   note All Regex Expectations are implicitly embedded within start and end markers: ^&lt;expression&gt;$. This means regular expressions are always assumed to match the full line. Use .* to explicitly match only at the end of (.*&lt;expression&gt; (regex)), or the start of (&lt;expression&gt;.* (regex)), or anywhere in (.*&lt;expression&gt;.* (regex)) a line. The regex Rust library that Scrut uses is an RE2 inspired engine with a very similar syntax. It most notably differs from Perl's PCRE in that it doesn't support backtracking (look-around) to ensure good performance.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#examples-3","content":" Expression\tMeaningHello.* (regex)\tA single output line that starts with Hello .*Hello.* (regex)\tA single output line that contains Hello .*Hello (regex)\tA single output line that ends with Hello .*Hello.* (regex?)\tAn optional output line that contains Hello .*Hello.* (regex*)\tAny amount (0..n) of output lines that contain Hello .*Hello.* (regex+)\tOne or more (1..n) of output lines that contain Hello Foo: [0-9]+ (regex+)\tOne or more (1..n) of output lines that start with Foo followed by a colon :, a whitespace and then only numbers till the end of the line  note You can use re as a shorthand for regex  ","version":"Next","tagName":"h3"},{"title":"Escaped Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#escaped-expectation","content":" CLIs usually only do (and mostly should) print out, well, printable characters. However, there are scenarios where you need to write binary data to STDOUT. More commonly you will encounter ANSI escape sequences for color coding and so forth. Lastly, consider the good old tab character \\t, which may be hard to read (or write) in a text editor.  Scrut tests live in text documents that are intended to be edited by users. They should not contain binary. To that end, any non-printable output can be denoted in it's hexadecimal escaped form \\xAB (with AB being the hexadecimal value of the bytecode of the character) or \\t to denote tab characters.  The following example shows an expectation of a string that renders as a bold, red font on the command line  # Colorful Fun ```scrut $ echo -e 'Foo \\033[1;31mBar\\033[0m Baz' Foo \\x1b[1mBar\\x1b[0m Baz (escaped) ```   Or consider some program that prints out two \\x00 separated strings:  # Colorful Fun ```scrut $ some-program foo\\x00bar (escaped) ```   Or again, the good old tab character:  # Love the CSV ```scrut $ csv-generator foo\\tbar\\tbaz (escaped) ```   note Newlines are ignored for Escaped Expectations. So foo\\tbar (escaped) matches both foo\\tbar\\n and foo\\tbar.  ","version":"Next","tagName":"h2"},{"title":"Examples‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#examples-4","content":" Expression\tMeaningHello\\tWorld (escaped)\tOne output line of that starts with Hello, followed by a tab character, followed by World Hello\\tWorld (escaped?)\tAn optional output line that contains Hello, followed by a tab character, followed by World Hello\\tWorld (escaped*)\tAny amount (0..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World Hello\\tWorld (escaped+)\tOne or more (1..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World  note You can use esc as a shorthand for escaped  ","version":"Next","tagName":"h3"},{"title":"Escaped Glob Expectations‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#escaped-glob-expectations","content":" Because it came up often enough, you can use (escaped) in combination with (glob):  # Glob Escaped Output ```scrut $ csv-generator foo\\t* (escaped) (glob+) bar\\tbaz (escaped) ```   The above exports one or more lines of output that start with foo followed by tab. The last line of output is expected to be bar, followed by tab, followed by baz.  Expression\tMeaningHello\\tWorld* (escaped) (glob)\tOne output line of that starts with Hello, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob?)\tAn optional output line that contains Hello, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob*)\tAny amount (0..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World, followed by anything Hello\\tWorld* (escaped) (glob+)\tOne or more (1..n) of output lines that contain Hello\\tWorld, followed by a tab character, followed by World, followed by anything  note You can use shorthands for either. Quantifiers must be always on glob.  ","version":"Next","tagName":"h3"},{"title":"Edge-Case: Output vs Expectations‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/reference/fundamentals/output-expectations/#edge-case-output-vs-expectations","content":" You may run into a case where you CLI output actually contains an a string that resembles an output expectation kind. For example, consider the following output:  Output $ my-cli --some arg Hello (equal)   The (equal) part above is part of the output of the CLI, not an output expectation of the kind equal. To account for that simply be specific with the rules. The following validates the output of the above execution:  tests/validate-output.md # Some Test that accounts for output expectations in output ```scrut $ my-cli --some arg Hello (equal) (equal) ```   The string Hello (equal) (equal) will be read by Scrut as following:  This is an equal output expectation, as signified by the suffix (equal)The expected output, that precedes the &quot;kind notation&quot;, is Hello (equal)If the output of the my-cli --some arg is exactly Hello (equal), then the test passes  Meaning: By giving Scrut the explicit (equal) suffix, it will be able to distinguish between the output expectation and the output itself. ","version":"Next","tagName":"h2"},{"title":"Inline Configuration","type":0,"sectionRef":"#","url":"/scrut/docs/reference/fundamentals/inline-configuration/","content":"","keywords":"","version":"Next"},{"title":"Example‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#example","content":" example.md --- # optional document-wide YAML configuration total_timeout: 30s --- # The test document The initial block that is initialized with `---` and terminated with `---` contains the configuration in YAML notation. ## A simple test ```scrut $ echo Hello One Hello One ``` The above test does not contain any per-test configuration ## A test with configuration ```scrut {timeout: 10s} $ echo Hello Two Hello Two ``` The above test contains per-test configuration   Some inline-configuration attribute can overwritten by parameters provided on the command-line (see below). The order of precedence is:  Command-line parameterPer Test Case configurationPer Test Document configurationDefault  ","version":"Next","tagName":"h2"},{"title":"Test Document Configuration‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#test-document-configuration","content":" All configuration that can be applied per test document.  ","version":"Next","tagName":"h2"},{"title":"append‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#append","content":" Type: list of paths to documentsCommand Line Parameter: --append-test-file-pathsDefault: []  The append configuration allows you to specify a list of document paths that should be included as if they were part of the current test document. All tests within the appended paths are executed after the tests defined in the current document. This is particularly useful for including common or shared test tear-down procedures. The paths specified must be relative to the current $TESTDIR.  Example:  append: - &quot;common-teardown.md&quot; - &quot;additional-tests.md&quot;   warning Per-document configuration, including defaults, from appended documents is ignored.  ","version":"Next","tagName":"h3"},{"title":"defaults‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#defaults","content":" Type: objectCommand Line Parameter: n/aDefault: {}  The defaults configuration allows you to specify default values for per-test-case configurations within the test document. These defaults are applied to each test case unless overridden by specific configurations within the test case itself. This is useful for setting common configurations that apply to multiple test cases, reducing redundancy and ensuring consistency across tests.  Example:  defaults: timeout: &quot;5s&quot; environment: FOO: &quot;bar&quot;   In the above example, each test case will have a default timeout of 5 seconds and an environment variable FOO set to &quot;bar&quot;, unless these are explicitly overridden in the test case configuration.  ","version":"Next","tagName":"h3"},{"title":"prepend‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#prepend","content":" Type: list of paths to documentsCommand Line Parameter: --prepend-test-file-pathsDefault: []  The prepend configuration allows you to specify a list of document paths that should be included as if they were part of the current test document. All tests within the prepended paths are executed before the tests defined in the current document. This is particularly useful for including common or shared test setup procedures. The paths specified must be relative to the current $TESTDIR.  Example:  prepend: - &quot;common-setup.md&quot; - &quot;initial-tests.md&quot;   warning Per-document configuration, including defaults, from prepended documents is ignored.  ","version":"Next","tagName":"h3"},{"title":"shell‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#shell","content":" Type: stringCommand Line Parameter: --shellDefault (Linux, MacOS): /bin/bashDefault (Windows): bash  The shell configuration specifies the path to the shell that should be used to execute the test cases. If a full path is not provided, the shell command must be available in the system's $PATH. Currently, only bash compatible shells are supported. This configuration is useful when you need to run tests in a specific shell environment that might have different features or behaviors compared to the default shell.  Example:  shell: /bin/my-bash   tip You can also overwrite the default shell using the SCRUT_DEFAULT_SHELL environment variable.  ","version":"Next","tagName":"h3"},{"title":"total_timeout‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#total_timeout","content":" Type: duration stringCommand Line Parameter: --timeout-secondsDefault: 15m  The total_timeout configuration specifies the maximum duration allowed for all tests within the document to complete execution. This includes tests from both appended and prepended documents. If the total execution time exceeds this limit, the test run is aborted. This setting is useful for ensuring that test suites do not run indefinitely and helps in managing overall test execution time.  Example:  total_timeout: &quot;30m&quot;   ","version":"Next","tagName":"h3"},{"title":"Test Case Configuration‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#test-case-configuration","content":" All configuration that can be applied per test case in Markdown test documents.  note Mind that Cram does not support per-test-case configuration and that defaults for Markdown and Cram have slightly different default values. If they differ then Markdown Default and Cram Default are provided below, if they are the same then only Default is mentioned.  ","version":"Next","tagName":"h2"},{"title":"detached‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#detached","content":" Type: booleanCommand Line Parameter: n/aDefault: false  Tell Scrut that the shell expression of this test will detach itself, so Scrut will not consider this a test (i.e. no output or exit code evaluation). Purpose is to run detached commands (like nohup some-command &amp;) that are doing something asynchronous (e.g. starting a server to which the tested CLI is a client).  Example:  ```scrut {detached: true} $ my-server --start ```   ","version":"Next","tagName":"h3"},{"title":"detached_kill_signal‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#detached_kill_signal","content":" Type: enum(disabled, SIGINT, int, 2, SIGABRT, abrt, 6, ...), see here for all supported namesCommand Line Parameter: n/aDefault: term  If detached is set to true then this configuration specifies the signal that is send to the detached process when all testcases in the test document have been executed.  Example:  ```scrut {detached: true, detached_kill_signal: term} $ my-server --start ```   warning Kill signals are only supported on Linux and MacOS. They are ignored (but validated) on Windows.  ","version":"Next","tagName":"h3"},{"title":"environment‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#environment","content":" Type: objectCommand Line Parameter: n/aDefault: {}  This configuration allows you to set environment variables for the test case. The environment variables are specified as key-value pairs in an object. These variables are set in the environment where the test case is executed.  Example:  ```scrut {environment: {&quot;FOO&quot;: &quot;bar&quot;}} $ echo $FOO bar ```   ","version":"Next","tagName":"h3"},{"title":"keep_crlf‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#keep_crlf","content":" Type: booleanCommand Line Parameter: --keep-output-crlfMarkdown Default: falseCram Default: true  This configuration determines whether carriage return and line feed (CRLF) sequences should be preserved in the output. When set to true, CRLF sequences are kept as-is, which is useful for tests that require exact output matching, including line endings. When set to false, CRLF sequences are translated to line feed (LF) only, which is the default behavior and is typically used for compatibility with Unix-style line endings.  Example:  ```scrut {keep_crlf: 42} $ echo -e &quot;Give CRLF\\r\\n&quot; Give CRLF\\r (escape) ```   ","version":"Next","tagName":"h3"},{"title":"output_stream‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#output_stream","content":" Type: enum(stdout, stderr, combined)Command Line Parameter: --combine-output and --no-combine-outputMarkdown Default: stdoutCram Default: combined  This configuration specifies which output stream to use when applying output expectations. The options are:  stdout: All expectations apply to what is printed on STDOUT.stderr: All expectations apply to what is printed on STDERR.combined: STDOUT and STDERR are combined into a single stream where all expectations are applied.  Example:  ```scrut {output_stream: combined} $ echo &quot;This goes to STDERR&quot; &gt;&amp;2 &amp;&amp; echo &quot;This goes to STDOUT&quot; This goes to STDERR This goes to STDOUT ```   ","version":"Next","tagName":"h3"},{"title":"skip_document_code‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#skip_document_code","content":" Type: positive integerCommand Line Parameter: n/aDefault: 80  This configuration specifies an exit code that, if returned by any test within the document, will cause the entire document to be skipped. This is useful for scenarios where a specific condition, indicated by the exit code, should prevent further tests from running. The default value is 80.  Example:  ```scrut {skip_document_code: 42} $ echo &quot;I give up&quot; &amp;&amp; exit 42 ```   ","version":"Next","tagName":"h3"},{"title":"strip_ansi_escaping‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#strip_ansi_escaping","content":" Type: booleanCommand Line Parameter: n/aDefault: false  This configuration determines whether ANSI escape sequences should be stripped from the CLI output before validation. When set to true, all ANSI escape sequences are removed, which is useful for tests that require output without formatting codes. When set to false, ANSI escape sequences are preserved, allowing for validation of formatted output.  Example:  example.md ```scrut {strip_ansi_escaping: true} $ echo -e &quot;\\033[31mThis is red text\\033[0m&quot; This is red text ```   ","version":"Next","tagName":"h3"},{"title":"timeout‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#timeout","content":" Type: duration stringCommand Line Parameter: n/aDefault: unset  The timeout configuration specifies the maximum duration allowed for a single test case to complete execution. If the test case does not finish within this time frame, it is aborted and the execution is considered an error. This setting is useful for ensuring that individual tests do not run indefinitely and helps in managing the execution time of each test case.  ```scrut {timeout: 5s} $ sleep 10 ```   ","version":"Next","tagName":"h3"},{"title":"wait‚Äã","type":1,"pageTitle":"Inline Configuration","url":"/scrut/docs/reference/fundamentals/inline-configuration/#wait","content":" Type: duration string, or {wait: {timeout: &lt;duration-string&gt;, path: &lt;path&gt;}}Command Line Parameter: n/aDefault: unset  This configuration is used to specify a waiting period for a test case, which is particularly useful in scenarios where a test needs to wait for a certain condition to be met before proceeding. The wait configuration can be set to a duration string to specify a simple wait time, or it can be a more complex configuration that includes both a timeout and a path condition. If the path is specified, the wait will end early if the specified path exists, allowing for synchronization with external processes or conditions.  Example (simple, only timeout):  ```scrut {wait: &quot;10s&quot;} $ echo &quot;Waiting for 10 seconds&quot; ```   Example (extended, timeout and path):  # Run something that creates `sock` file ```scrut {detached: true} $ start-something --sock-file &quot;$TMPDIR/sock&quot; ``` The above executes `start-something` that creates the file `$TMPDIR/sock` when it is ready. # Wait at most 10 seconds or until `sock` file exists ```scrut {wait: {timeout: &quot;10s&quot;, path: &quot;sock&quot;}} $ echo &quot;Can work with $TMPDIR/sock now&quot; ``` The above waits for `$TMPDIR/sock` to exist for at most 10 seconds.  ","version":"Next","tagName":"h3"},{"title":"Basic Expectations","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/basic-expectations/","content":"","keywords":"","version":"Next"},{"title":"Ignore Command Output‚Äã","type":1,"pageTitle":"Basic Expectations","url":"/scrut/docs/tutorial/basic-expectations/#ignore-command-output","content":" Consider how you would get rid of the output when executing the command jq --version on the shell. You would likely do something this:  Terminal $ jq --version &gt; /dev/null   info The suffix &gt; /dev/null redirects the output that jq --version writes to STDOUT to /dev/null, resulting in nothing being written to STDOUT. As STDERR is not considered by default this is sufficient.  And this is exactly what needs to be changed in the test document:  tests/test.md # Command executes successfully ```scrut $ jq --version &gt; /dev/null ```   With the output expectation removed this test will do as a smoke test.  ","version":"Next","tagName":"h2"},{"title":"Exit Code Validation by Default‚Äã","type":1,"pageTitle":"Basic Expectations","url":"/scrut/docs/tutorial/basic-expectations/#exit-code-validation-by-default","content":" Scrut automatically validates that the exit code of the execution of the shell expression is 0 which signifies that the execution ended without any failure. If it is not 0, then the execution is considered a failure and the validation of the test case will fail.  That means: We are already testing if it does &quot;blow up&quot;, as Scrut would fail the test if the execution blows up and ends in a non-zero exit code.  To make this clear, consider the following document of a test that will fail:  tests/fail.md # Test will fail ```scrut $ false ```   info The false command always fails and exits with a the exit code 1.  And here is how Scrut would tell you about the failure:  Terminal $ scrut test tests/fail.md üîé Found 1 test document(s) ‚ùå tests/fail.md: failed 1 out of 1 testcase // ============================================================================= // @ tests/fail.md:4 // ----------------------------------------------------------------------------- // # Test will fail // ----------------------------------------------------------------------------- // $ false // ============================================================================= unexpected exit code expected: 0 actual: 1 ## STDOUT ## STDERR Result: 1 document(s) with 1 testcase(s): 0 succeeded, 1 failed and 0 skipped   ","version":"Next","tagName":"h2"},{"title":"Expect a Non-Zero Exit Code‚Äã","type":1,"pageTitle":"Basic Expectations","url":"/scrut/docs/tutorial/basic-expectations/#expect-a-non-zero-exit-code","content":" If the shell expression that is being tested is actually expected to return a non-zero exit code, then the [&lt;exit-code&gt;] expectation can be used to communicate this to Scrut. Here an example:  fail.md # Test will fail ```scrut $ false [1] ```   The [1] signifies that the test validation should expect an exit code of 1. Now the above document is valid again:  Terminal $ scrut test tests/fail.md üîé Found 1 test document(s) Result: 1 document(s) with 1 testcase(s): 1 succeeded, 0 failed and 0 skipped   If any different number than 1 would have been set then the validation would fail.  note Scrut automatically assumes 0 exit code by default. Specifying it with [0] is not needed (but also not invalid). ","version":"Next","tagName":"h2"},{"title":"Test Creation","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/create-test/","content":"","keywords":"","version":"Next"},{"title":"Using Scrut's Built-in Test Creation‚Äã","type":1,"pageTitle":"Test Creation","url":"/scrut/docs/tutorial/create-test/#using-scruts-built-in-test-creation","content":" Generating a Scrut test from the command line is pretty straight forward:  Terminal $ scrut create --output tests/smoke.md -- jq --version ‚úçÔ∏è /tmp/smoke.md: Writing generated test document   This will create a test file that should look like this:  tests/smoke.md # Command executes successfully ```scrut $ jq --version jq-1.7.1 ```   You can now execute the newly created test file with:  Terminal $ scrut test tests/smoke.md üîé Found 1 test document(s) Result: 1 document(s) with 1 testcase(s): 1 succeeded, 0 failed and 0 skipped   note The scrut test command accepts arbitrary files or directories. All of the following (assuming the paths exist) are valid: scrut test tests - test every test file found (recursively) in testsscrut test tests/smoke.md tests/other.md - test both files tests/smoke.md and tests/other.mdscrut test tests other-tests - test all files found (recursively) in the tests and other-tests directories  ","version":"Next","tagName":"h2"},{"title":"Use STDIN to receive commands‚Äã","type":1,"pageTitle":"Test Creation","url":"/scrut/docs/tutorial/create-test/#use-stdin-to-receive-commands","content":" Alternatively you can also pipe the command via STDIN to scrut create:  Terminal $ echo &quot;jq --version&quot; | scrut create - &gt; tests/smoke.md ‚úçÔ∏è STDOUT: Writing generated test document   Here also --output was omitted, in which case scrut create will print the newly created test file to STDOUT. Check out scrut create --help to see all options.  ","version":"Next","tagName":"h3"},{"title":"Write tests manually‚Äã","type":1,"pageTitle":"Test Creation","url":"/scrut/docs/tutorial/create-test/#write-tests-manually","content":" You can of course also create your tests/smoke.md file manually in a text editor. As Scrut test documents are written in Markdown any Markdown syntax highlighting plugin for your IDE of choice will help greatly. ","version":"Next","tagName":"h2"},{"title":"Next Up","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/next-up/","content":"Next Up Well done! You have learned how to use Scrut to write tests for your CLI. Here are your next steps: Dive into the Reference and learn more about: How you can use inline configuration per-document or per-test-caseHow the Markdown Format differs from the Cram FormatWhat other environment variables are availableHow to conditionally skip tests using exit codesThe additional output expectations that existHow Scrut executes tests under the hoodand much more Learn how to integrated Scrut in your CI/CD Running Scrut in Docker ContainerScrut as a GitHub Action","keywords":"","version":"Next"},{"title":"Test Environment","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/test-environment/","content":"","keywords":"","version":"Next"},{"title":"Test Fixtures‚Äã","type":1,"pageTitle":"Test Environment","url":"/scrut/docs/tutorial/test-environment/#test-fixtures","content":" The reason why glob was used was that the output of curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' was simply not stable: The output is changing over time.  However, there is another alternative to stabilize the test: Storing the output of the curl command in a file and using the file contents as input for jq. A classic test fixture. This way the test input will not change, making the test stable, while retaining the higher precision.  First create a new file expectations-fixture.txt in the same directory as expectations.md and add the following:  tests/expectations-fixture.txt 2025-03-28T00:57:51Z,dependabot[bot] 2025-03-28T00:56:51Z,dependabot[bot] 2025-03-28T00:55:39Z,dependabot[bot] 2025-03-27T23:43:06Z,itchyny 2025-03-27T23:42:44Z,itchyny   Now with that in place, the expectations.md test document can be updated to use the fixture instead of the curl command:  tests/expectations.md # Output Expectations ```scrut $ cat &quot;$TESTDIR&quot;/expectations-fixture.txt | \\ &gt; jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' 2025-03-28T00:57:51Z,dependabot[bot] 2025-03-28T00:56:51Z,dependabot[bot] 2025-03-28T00:55:39Z,dependabot[bot] 2025-03-27T23:43:06Z,itchyny 2025-03-27T23:42:44Z,itchyny ```   note Test fixtures are extremely helpful to increase test isolation. Using the fixture above categorically removed internet access, github.com availability and curl availability and functionality as dependencies from the test.  ","version":"Next","tagName":"h2"},{"title":"Environment Variables‚Äã","type":1,"pageTitle":"Test Environment","url":"/scrut/docs/tutorial/test-environment/#environment-variables","content":" The TESTDIR environment variable that is now used in expectations.md is a special variable that is automatically set by Scrut. It contains the path of the directory where the test file that is currently being executed is located.  Two other very useful environment variables are:  TMPDIR: A temporary directory that is created for every Scrut execution automatically. It is removed when all tests are done.SCRUT_TEST: Is set to contain the path and the line number where the where the test case is located in the test document (.e.g dir/test.md:123)  info Learn about all environment variables that Scrut maintains.  ","version":"Next","tagName":"h2"},{"title":"Working Directory‚Äã","type":1,"pageTitle":"Test Environment","url":"/scrut/docs/tutorial/test-environment/#working-directory","content":" Scrut creates a temporary directory for each test document that is processed in scrut test or scrut update. This directory becomes the current working directory (CWD) for the test execution.  This working directory directory does not contain test documents or test fixtures. This is the reason why the environment variable TESTDIR was earlier used to cat the fixture file:  ```scrut $ cat &quot;$TESTDIR&quot;/expectations-fixture.txt | \\ &gt; jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' -- %&lt; -- ```   While the working directory is temporary, it is still shared between all test cases. So files can be written into it and picked up by later test cases. However, the above mentioned TMPDIR environment variable is even better for that.  To access the current directory either execute pwd or access the PWD environment variable:  ```scrut $ echo &quot;I am in $(pwd), which is not $TESTDIR&quot; ```   info Learn more about the working directory in Reference &gt; Behavior &gt; Working Directory. ","version":"Next","tagName":"h2"},{"title":"Test Configuration","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/test-configuration/","content":"","keywords":"","version":"Next"},{"title":"Inline Configuration‚Äã","type":1,"pageTitle":"Test Configuration","url":"/scrut/docs/tutorial/test-configuration/#inline-configuration","content":" The preferred way is to persist the modification of the behavior in the test document itself. This way you can easily share the test document across test execution environments.  A common scenario is constraining the maximal execution time of individual test cases. For that purpose Scrut provides two timeout configuration options:  Per Test Document, constraining the cumulative execution time of all test cases in the test document.Per Test Case, constraining only the execution time of a single test case.  info There are plenty more configuration options that are described in Reference &gt; Fundamentals &gt; Inline Configuration.  ","version":"Next","tagName":"h2"},{"title":"Per-Test-Document Configuration‚Äã","type":1,"pageTitle":"Test Configuration","url":"/scrut/docs/tutorial/test-configuration/#per-test-document-configuration","content":" The per-test-document timeout is defined in the test document's front matter (&quot;a YAML snippet at the top of the Markdown document&quot;). The following example leads to aborted test execution if the sum of executing all test cases exceeds 30 seconds.  tests/timeout.md --- total_timeout: 30s --- # Test One ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' * (glob+) ``` # Test Two ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .some-other | valid-expression' * (glob+) ```   ","version":"Next","tagName":"h3"},{"title":"Per-Test-Case Configuration‚Äã","type":1,"pageTitle":"Test Configuration","url":"/scrut/docs/tutorial/test-configuration/#per-test-case-configuration","content":" Per-testcase configuration allows for much more granular control. This configuration must be provided as one-line YAML that is wrapped in curly brackets {}. The following example constraints the first test case execution to 10 seconds and the second one to twenty seconds.  tests/timeout.md # Test One ```scrut {timeout: 10s} $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' * (glob+) ``` # Test Two ```scrut {timeout: 20s} $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .some-other | valid-expression' * (glob+) ```   ","version":"Next","tagName":"h3"},{"title":"Default Per-Test-Case Timeout‚Äã","type":1,"pageTitle":"Test Configuration","url":"/scrut/docs/tutorial/test-configuration/#default-per-test-case-timeout","content":" If per-test-case configuration is shared within the document, then you can also overwrite default values in the per-document section of the configuration. The following sets a default timeout of 10 seconds for each test case, which is then later overwritten by the second test case to again 20 seconds:  tests/timeout.md --- defaults: timeout: 10s --- # Test One ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' * (glob+) ``` # Test Two ```scrut {timeout: 20s} $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .some-other | valid-expression' * (glob+) ```   ","version":"Next","tagName":"h3"},{"title":"Command Line Parameters‚Äã","type":1,"pageTitle":"Test Configuration","url":"/scrut/docs/tutorial/test-configuration/#command-line-parameters","content":" Many of the configuration options are mirrored by command line parameters. The per-document timeout described above could also be set using --timeout-seconds=30 for the scrut test command. However, there is no equivalent for the timeout per-test-case configuration.  You can review all the available parameters by executing scrut test --help and respective scrut update --help.  warning Using command line parameters breaks the encapsulation of the test documents. That means in order to replicate the test execution you need to know the command line arguments that were passed to scrut test - and their order! ","version":"Next","tagName":"h2"},{"title":"Test Bootstrapping","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/test-bootstrapping/","content":"","keywords":"","version":"Next"},{"title":"Bootstrap the Environment‚Äã","type":1,"pageTitle":"Test Bootstrapping","url":"/scrut/docs/tutorial/test-bootstrapping/#bootstrap-the-environment","content":" Mind that shell expressions are just that: arbitrary expressions that are interpreted and executed by a bash process. If a long expression would be repeatedly used manually on the command line it would not take long until someone is fed up and creates an alias. This is exactly what you should do in a Scrut test document:  boostrapped.md # Validate many closely related things Setup ```scrut $ alias jq_run='jq -r -M -S' ``` ## Validate a thing ```scrut $ cat &quot;$TESTDIR&quot;/some-fixture.txt | \\ &gt; jq_run '.some | jq(expression)' ``` ## Validate another thing ```scrut $ cat &quot;$TESTDIR&quot;/other-fixture.txt | \\ &gt; jq_run '.another | jq(expression)' ``` etc   note You can use alias directly, because Scrut Markdown test execution already sets shopt -s expand_aliases for you. In Cram tests you will need to set that yourself. Alternatively you can create a bash function or introduce an environment variable. The choice is yours. # Alternative Setup ```scrut $ export JQ_RUN='jq -r -M -S' ``` # Alternative Usage ```scrut $ cat &quot;$TESTDIR&quot;/other-fixture.txt | \\ &gt; $JQ_RUN '.another | jq(expression)' ```   The new test case that was inserted at the top is not strictly speaking a test for jq. However, it helps to make the test document more readable and maintainable.  ","version":"Next","tagName":"h2"},{"title":"Share Setup between Test Documents‚Äã","type":1,"pageTitle":"Test Bootstrapping","url":"/scrut/docs/tutorial/test-bootstrapping/#share-setup-between-test-documents","content":" While the alias at the top of the document simplifies writing, reading, and maintaining tests within a single document, it can become repetitive across multiple documents. If your setup logic extends beyond a single alias statement, this repetition can become cumbersome.  To make the setup reusable, you can move it to a separate file. Create a file tests/setup.sh with the following:  tests/setup.sh #!/bin/bash alias jq_run='jq -r -M -S'   Now all you need to do is included that setup.sh file in your test document. You would do it in the same way as you would in bash, that is using source to execute and load it into the current shell process:  boostrapped.md # Validate many closely related things Setup ```scrut $ source &quot;$TESTDIR&quot;/setup.sh ``` ## Validate a thing ```scrut $ cat &quot;$TESTDIR&quot;/some-fixture.txt | \\ &gt; jq_run '.some | jq(expression)' ``` ## Validate another thing ```scrut $ cat &quot;$TESTDIR&quot;/other-fixture.txt | \\ &gt; jq_run '.another | jq(expression)' ``` etc   ","version":"Next","tagName":"h2"},{"title":"Prepending and Appending Test Documents‚Äã","type":1,"pageTitle":"Test Bootstrapping","url":"/scrut/docs/tutorial/test-bootstrapping/#prepending-and-appending-test-documents","content":" Another pattern that is related to the above is to prepend and append whole test documents. This makes especially sense if you have a large amount of shared test cases that you want to reuse across many test documents.  Scrut offers the prepend and append per-test-document configuration options for that purpose. Consider the following test document:  tests/setup.md # Setup ```scrut $ source &quot;$TESTDIR&quot;/setup.sh ```   Here is how you can prepend all test cases in the above document (there could be more than just the one) to the test cases found in another document:  tests/bootstrapped.md --- prepend: - tests/setup.md --- # Validate many closely related things ## Validate a thing ```scrut $ cat &quot;$TESTDIR&quot;/some-fixture.txt | \\ &gt; jq_run '.some | jq(expression)' ``` ## Validate another thing ```scrut $ cat &quot;$TESTDIR&quot;/other-fixture.txt | \\ &gt; jq_run '.another | jq(expression)' ``` etc   note The paths in prepend and append must be relative to the current TESTDIR.The order matters! The files will be prepended / appended as provided.  You can conceptualize these options as &quot;test setup&quot; (prepend) and &quot;test teardown&quot; (append) as seen in other testing frameworks.  ","version":"Next","tagName":"h2"},{"title":"Alternative: Command-line Arguments‚Äã","type":1,"pageTitle":"Test Bootstrapping","url":"/scrut/docs/tutorial/test-bootstrapping/#alternative-command-line-arguments","content":" The scrut test command provides two parameters that are similar to the prepend and append configuration options:  --prepend-test-file-paths / -P: Optional list of paths to test files which are prepended to each test file in execution.--append-test-file-paths / -A: Optional list of paths to test files which are appended to each test file in execution.  The difference to configuration options is that parameters are applied to all test documents. So if you execute scrut test tests/ (i.e. all tests in a folder as oppose to a single test document) then the specified test document(s) will be prepended/append to all test documents in that folder.  Here is how that looks in practice:  $ scrut test -P tests/setup.md tests/boostrapped.md   warning Use configuration instead of command-line parameters whenever possible, so to not break the test isolation. ","version":"Next","tagName":"h3"},{"title":"Output Expectations","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/output-expectations/","content":"","keywords":"","version":"Next"},{"title":"Output Expectation Types‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/tutorial/output-expectations/#output-expectation-types","content":" The simplest variant of an output expectation was already demonstrated previously when the test for the jq --version command was created:  ```scrut $ jq --version jq-1.7.1 ```   The line that reads jq-1.7.1 is what Scrut calls a equal output expectation. It could also have been written like this:  ```scrut $ jq --version jq-1.7.1 (equal) ```   The suffix (equal) here tells Scrut that the output is expected exactly as written before. There are other types, for example:  Glob: Match all  ```scrut $ jq --version jq-* (glob) ```   The * wildcard in jq-* matches anything. Scrut would accept any string that starts with jq-.  Regex: Match precisely  ```scrut $ jq --version jq-1\\.\\d+\\.\\d+ (regex) ```   The 1\\.\\d+\\.\\d+ regular expression matches any version number that starts with a one and is followed by two numbers, separated by a dot.  info Learn more about output expectations in the Reference &gt; Fundamentals &gt; Output Expectations later.  ","version":"Next","tagName":"h2"},{"title":"Practical Example‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/tutorial/output-expectations/#practical-example","content":" Let's take a look at a practical example. Using jq some JSON input data is required. Following the same example as provided in the jq tutorial itself: Let's go with the Github API.  Terminal $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' [ { &quot;sha&quot;: &quot;947fcbbb1fedbdd6021ef3f93782a500e32d5dcd&quot;, &quot;node_id&quot;: &quot;C_kwDOAE3WVdoAKDk0N2ZjYmJiMWZlZGJkZDYwMjFlZjNmOTM3ODJhNTAwZTMyZDVkY2Q&quot;, &quot;commit&quot;: { &quot;author&quot;: { &quot;name&quot;: &quot;dependabot[bot]&quot;, &quot;email&quot;: &quot;49699333+dependabot[bot]@users.noreply.github.com&quot;, &quot;date&quot;: &quot;2025-03-28T00:57:51Z&quot; }, &quot;committer&quot;: { &quot;name&quot;: &quot;GitHub&quot;, &quot;email&quot;: &quot;noreply@github.com&quot;, &quot;date&quot;: &quot;2025-03-28T00:57:51Z&quot; }, &quot;message&quot;: &quot;--%&lt;--&quot;, &quot;tree&quot;: { &quot;sha&quot;: &quot;8b30ae1036b74c4acf02c674f75db8f1ce014aa4&quot;, &quot;url&quot;: &quot;https://api.github.com/repos/jqlang/jq/git/trees/8b30ae1036b74c4acf02c674f75db8f1ce014aa4&quot; }, &quot;url&quot;: &quot;https://api.github.com/repos/jqlang/jq/git/commits/947fcbbb1fedbdd6021ef3f93782a500e32d5dcd&quot;, &quot;comment_count&quot;: 0, &quot;verification&quot;: { &quot;verified&quot;: true, &quot;reason&quot;: &quot;valid&quot;, &quot;signature&quot;: &quot;--%&lt;--&quot;, &quot;payload&quot;: &quot;--%&lt;--&quot;, &quot;verified_at&quot;: &quot;2025-03-28T00:57:55Z&quot; } }, &quot;url&quot;: &quot;https://api.github.com/repos/jqlang/jq/commits/947fcbbb1fedbdd6021ef3f93782a500e32d5dcd&quot;, &quot;html_url&quot;: &quot;https://github.com/jqlang/jq/commit/947fcbbb1fedbdd6021ef3f93782a500e32d5dcd&quot;, &quot;comments_url&quot;: &quot;https://api.github.com/repos/jqlang/jq/commits/947fcbbb1fedbdd6021ef3f93782a500e32d5dcd/comments&quot;, &quot;author&quot;: { --%&lt;--   That is a lot of output. Let's use jq to boil that down into something more manageable. Say, as CSV with the first column the commit date and the second column the author's name:  Terminal $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' 2025-03-28T00:57:51Z,dependabot[bot] 2025-03-28T00:56:51Z,dependabot[bot] 2025-03-28T00:55:39Z,dependabot[bot] 2025-03-27T23:43:06Z,itchyny 2025-03-27T23:42:44Z,itchyny   note The output you will see when executing the above curl command will contain more lines than are shown above: Terminal $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 28935 100 28935 0 0 250k 0 --:--:-- --:--:-- --:--:-- 250k 2025-03-28T00:57:51Z,dependabot[bot] 2025-03-28T00:56:51Z,dependabot[bot] 2025-03-28T00:55:39Z,dependabot[bot] 2025-03-27T23:43:06Z,itchyny 2025-03-27T23:42:44Z,itchyny The first three lines above that curl prints are written to STDERR. Only the actual result content (i.e. the web request body) is printed to STDOUT and piped to jq which transforms them into five lines that are finally printed on STDOUT. Scrut only considers STDOUT by default. More about how to change this behavior here.  To go from here to a test either use scrut create with the above command, or open a new file and add the commandline and output yourself:  tests/expectations.md # Output Expectations ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ &gt; jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' 2025-03-28T00:57:51Z,dependabot[bot] 2025-03-28T00:56:51Z,dependabot[bot] 2025-03-28T00:55:39Z,dependabot[bot] 2025-03-27T23:43:06Z,itchyny 2025-03-27T23:42:44Z,itchyny ```   note Shell expressions that span multiple lines need to be prefixed with a &gt;, like so: ```scrut $ line 1 &gt; line 2 &gt; line N ``` The whole expression will then be piped into a bash process and executed. If you do not concatenate the lines with something &amp;&amp; or explicitly set -e, then the exit code will be from the last executed line. ```scrut $ line 1 &amp;&amp; \\ &gt; line 2 &amp;&amp; \\ &gt; line N ```   ","version":"Next","tagName":"h2"},{"title":"Generalize Output Expectation‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/tutorial/output-expectations/#generalize-output-expectation","content":" Running scrut test tests/expectations.md right after creating the file should succeed. Should, because the output is not stable. It is not guaranteed to be the same tomorrow, or even in a few minutes. To make it more stable the test can be changed:  from with the JSON input from the github API this exact output is expectedto with the JSON input from the github API 5 lines separated by a comma are expected  ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ &gt; jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' *,* (glob) *,* (glob) *,* (glob) *,* (glob) *,* (glob) ```   Obviously this test lost precision compared to the previous variant, but on the plus side: it won't break as easy, it is still meaningful and it could break if, say, the jq concatenate operator + malfunctions. This could be made more precise using 20*T*Z,* (glob) to account for the date string, or even use matching regex rules.  ","version":"Next","tagName":"h3"},{"title":"Quantifiers for Expectations‚Äã","type":1,"pageTitle":"Output Expectations","url":"/scrut/docs/tutorial/output-expectations/#quantifiers-for-expectations","content":" The above test could be generalized further. While it probably would not make sense for this case the following would work as well:  ```scrut $ curl 'https://api.github.com/repos/jqlang/jq/commits?per_page=5' | \\ &gt; jq -r '.[] | .commit.committer.date + &quot;,&quot; + .commit.author.name' *,* (glob+) ```   Note the + behind the word glob. This is a quantifier. Quantifiers can be used with any output expectation. They make sense when a hard to predict amount of predictable formatted output needs to be accounted for.  note Scrut currently understands three quantifiers: ?: Zero or one*: Any amount, including zero+: Any amount, but at least one More detail in Reference &gt; Fundamentals &gt; Output Expectations &gt; Quantifiers. ","version":"Next","tagName":"h3"},{"title":"What to test?","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/what-to-test/","content":"What to test? CLIs are as diverse as the tasks they are designed to perform. The tests that validate an individual CLI must be tailored to the specific features and use-cases. On a very high level Scrut is designed to support the following types of tests: Smoke Testing - Validate that the CLI is fundamentally executable.Functional Testing - Validate that the CLI performs its intended functions.Integration / End-to-End Testing - Validate that the CLI works correctly when integrated with other systems. So where do you start? What is good first test to write in any case? All CLI share at least one trait: they all are have an interface to execute on the command line. So &quot;being executable from the command line&quot; is the testable trait. The type of test that addresses that, and therefore the aforementioned Smoke test is the best place to start. info A smoke test, in very short, is a very basic test that answers the question: If I switch it on, do I see smoke rising? Where it refers to an arbitrary electric device. For CLIs a smoke test means: If I execute the CLI in the most basic way does it return a 0 exit code? What is a &quot;basic way&quot;? Think about mycli --version or mycli --help. Both parameters should exist for a well written CLI and executing either should not run any significant business logic. This is what makes them great candidates for a smoke test. The value of testing such basic functionality of your CLI is that if the smoke test fails, then you know your CLI is fundamentally broken. For the scope of this tutorial: Wether jq --version can be executed successfully is the first test we will write.","keywords":"","version":"Next"},{"title":"Test Maintenance","type":0,"sectionRef":"#","url":"/scrut/docs/tutorial/test-maintenance/","content":"","keywords":"","version":"Next"},{"title":"Update Tests automatically‚Äã","type":1,"pageTitle":"Test Maintenance","url":"/scrut/docs/tutorial/test-maintenance/#update-tests-automatically","content":" The scrut update command can be run on one or multiple test documents to re-execute tests and automatically update the output expectations.  For instance, consider the smoke.md document, now renamed to version-test.md. Previously considered a smoke test, let's treat it as a test for the CLI output for now:  tests/version-test.md # Command executes successfully ```scrut $ jq --version jq-1.7.1 ```   Let's say it was written in the past with an earlier version of jq, hence it looks like this:  tests/version-test.md # Command executes successfully ```scrut $ jq --version jq-1.7.0 ```   Now executing this test with the jq version 1.7.1 will fail:  Terminal $ scrut test tests/version-test.md üîé Found 1 test document(s) ‚ùå tests/version-test.md: failed 1 out of 1 testcase // ============================================================================= // @ tests/version-test.md:4 // ----------------------------------------------------------------------------- // # Command executes successfully // ----------------------------------------------------------------------------- // $ jq --version // ============================================================================= 1 | - jq-1.7.0 1 | + jq-1.7.1 Result: 1 document(s) with 1 testcase(s): 0 succeeded, 1 failed and 0 skipped   As expected, the version strings do not match and the Scrut test fails. Now, now can obviously straighten that out speedily with a text editor by just changing jq-1.7.0 to jq-1.7.1, or by using glob or regex expectations. However, if you had tens of test files this would quickly become an annoying chore.  The scrut update command is a better way.  What it does is simply this:  Execute all tests, the same way scrut test wouldFor any that is invalid (i.e. that has output expectations that do not validate anymore) it offers you to write an updated test with fixed output expectations.  Here is how that looks in practice:  Terminal $ scrut update --replace tests/version-test.md üîé Found 1 test document(s) // @ tests/version-test.md:4 // ----------------------------------------------------------------------------- // # Command executes successfully // ----------------------------------------------------------------------------- // $ jq --version // ============================================================================= 1 | - jq-1.7.0 1 | + jq-1.7.1 ? Overwrite existing document tests/version-test.md? (y/n) ‚Ä∫ no   The above is an interactive dialog and waits for user input whether to overwrite the existing file. If consent is given then it would overwrite tests/version-test.md with the output that was received from executing jq --version.  note Useful parameters for scrut update are: --replace or -r, which writes the updated document into the same location as the original file. If not set then a file &lt;document-path&gt;.new will be created.--assume-yes or -y, which skips the confirmation and always assumes yes Check out scrut update --help for additional parameters.  warning There are limits to what scrut update can do: Only equal and escaped expectations can be updated, but glob and regex would be replaced with either equal or escaped, whichever fits.Quantifiers (*, +, ?) will not be considered: If an output expectation uses quantifiers and became invalid, then scrut update would write individual output expectations instead of one &quot;quantified&quot; expectation.Prepended and appended test documents are not updated (but you can update them individually) ","version":"Next","tagName":"h2"}]